{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1999104",
   "metadata": {},
   "source": [
    "# Model A: Protein Generator Diffusion model conditioned on overall secondary structure content\n",
    "\n",
    "### Generative method to design novel proteins using a diffusion model \n",
    "\n",
    "B. Ni, D.L. Kaplan, M.J. Buehler, Generative design of de novo proteins based on secondary structure constraints using an attention-based diffusion model, Chem, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a281c29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 10:51:32.188076: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-29 10:51:32.226664: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-29 10:51:32.226698: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-29 10:51:32.227672: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-29 10:51:32.233114: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-29 10:51:32.233772: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-29 10:51:33.003272: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/maxwell/anaconda3/envs/diff/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import math\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #turn off CUDA if needed\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # select which GPU device is to be used\n",
    "\n",
    "import shutil\n",
    "\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24aadd64-6124-4f25-9789-1067a878c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBParser\n",
    "from Bio.PDB.DSSP import DSSP\n",
    "from Bio.PDB import PDBList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e62b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#from imagen_pytorch import Unet, Imagen\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b85dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "\n",
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9183943f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16260708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924ee37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "num_of_gpus = torch.cuda.device_count()\n",
    "print(num_of_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "093f91f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed04b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c80f92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "from functools import partial, wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1daa07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b0d9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b0ff262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params (model):\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    pytorch_total_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print (\"Total parameters: \", pytorch_total_params,\" trainable parameters: \", pytorch_total_params_trainable)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9250f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionDataset(Dataset):\n",
    "\n",
    "        def __init__(self, X_data, y_data):\n",
    "            self.X_data = X_data\n",
    "            self.y_data = y_data\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.X_data[index], self.y_data[index]\n",
    "\n",
    "        def __len__ (self):\n",
    "            return len(self.X_data)\n",
    "\n",
    "def load_data_set_seq2seq (file_path , \n",
    "                   min_length=0, max_length=100, batch_size_=4, output_dim=1, maxdata=9999999999999,\n",
    "                           tokenizer_y=None, split=.2,):\n",
    "\n",
    "    min_length_measured=0\n",
    "    max_length_measured=999999\n",
    "    protein_df=pd.read_csv(file_path)\n",
    "    protein_df\n",
    "\n",
    "    protein_df.describe()\n",
    "     \n",
    "    df_isnull = pd.DataFrame(round((protein_df.isnull().sum().sort_values(ascending=False)/protein_df.shape[0])*100,1)).reset_index()\n",
    "    df_isnull.columns = ['Columns', '% of Missing Data']\n",
    "    df_isnull.style.format({'% of Missing Data': lambda x:'{:.1%}'.format(abs(x))})\n",
    "    cm = sns.light_palette(\"skyblue\", as_cmap=True)\n",
    "    df_isnull = df_isnull.style.background_gradient(cmap=cm)\n",
    "    df_isnull\n",
    "     \n",
    "    protein_df.drop(protein_df[protein_df['Seq_Len'] >max_length-2].index, inplace = True)\n",
    "    protein_df.drop(protein_df[protein_df['Seq_Len'] <min_length].index, inplace = True)\n",
    "\n",
    "    print(protein_df.shape)\n",
    "    print(protein_df.head(6))\n",
    "    protein_df=protein_df.reset_index(drop=True)\n",
    "    \n",
    "    seqs = protein_df.Sequence.values\n",
    "    \n",
    "    test_seqs = seqs[:1]\n",
    "  \n",
    "    lengths = [len(s) for s in seqs]\n",
    "\n",
    "    print(protein_df.shape)\n",
    "    print(protein_df.head(6))\n",
    "    \n",
    "    min_length_measured =   min (lengths)\n",
    "    max_length_measured =   max (lengths)\n",
    "\n",
    "    print (min_length_measured,max_length_measured)\n",
    "\n",
    "    fig_handle = sns.distplot(lengths,bins=50,kde=False, rug=False,norm_hist=False,axlabel='Length')\n",
    "    fig = fig_handle.get_figure()\n",
    "    plt.show()\n",
    "    \n",
    "    #INPUT - X  \n",
    "    X=[]\n",
    "    \n",
    "    for i in range  (len(seqs)):\n",
    "            \n",
    "            X.append([protein_df['AH'][i],protein_df['BS'][i],protein_df['T'][i],\n",
    "                     protein_df['UNSTRUCTURED'][i],protein_df['BETABRIDGE'][i],protein_df['310HELIX'][i],\n",
    "                      protein_df['PIHELIX'][i],\n",
    "                     protein_df['BEND'][i]])\n",
    "           \n",
    "    X=np.array(X)\n",
    "    print (\"sample X data\", X[0])\n",
    "    \n",
    "    fig_handle = sns.distplot(X[:,0],bins=50,kde=False, rug=False,norm_hist=False,axlabel='AH')\n",
    "    fig = fig_handle.get_figure()\n",
    "    plt.show()\n",
    "    fig_handle = sns.distplot(X[:,1],bins=50,kde=False, rug=False,norm_hist=False,axlabel='BS')\n",
    "    fig = fig_handle.get_figure()\n",
    "    plt.show()\n",
    "    fig_handle = sns.distplot(X[:,2],bins=50,kde=False, rug=False,norm_hist=False,axlabel='T')\n",
    "    fig = fig_handle.get_figure()\n",
    "    plt.show()\n",
    "    fig_handle = sns.distplot(X[:,3],bins=50,kde=False, rug=False,norm_hist=False,axlabel='UNSTRUCTURED')\n",
    "    fig = fig_handle.get_figure()\n",
    "    plt.show()\n",
    "    fig_handle = sns.distplot(X[:,4],bins=50,kde=False, rug=False,norm_hist=False,axlabel='BETABRIDGE')\n",
    "    fig = fig_handle.get_figure()\n",
    "    plt.show()\n",
    "    fig_handle = sns.distplot(X[:,5],bins=50,kde=False, rug=False,norm_hist=False,axlabel='310HELIX')\n",
    "    fig = fig_handle.get_figure()\n",
    "    plt.show()\n",
    "    fig_handle = sns.distplot(X[:,6],bins=50,kde=False, rug=False,norm_hist=False,axlabel='PIHELIX')\n",
    "    fig = fig_handle.get_figure()\n",
    "    plt.show()\n",
    "    fig_handle = sns.distplot(X[:,7],bins=50,kde=False, rug=False,norm_hist=False,axlabel='BEND')\n",
    "    fig = fig_handle.get_figure()\n",
    "    plt.show()    \n",
    "\n",
    "    seqs = protein_df.Sequence.values\n",
    "            \n",
    "    #create and fit tokenizer for AA sequences\n",
    "    if tokenizer_y==None:\n",
    "        tokenizer_y = Tokenizer(char_level=True, filters='!\"$%&()*+,-./:;<=>?[\\\\]^_`{|}\\t\\n' )\n",
    "        tokenizer_y.fit_on_texts(seqs)\n",
    "        \n",
    "    y_data = tokenizer_y.texts_to_sequences(seqs)\n",
    "    \n",
    "    y_data= sequence.pad_sequences(y_data,  maxlen=max_length, padding='post', truncating='post')  \n",
    " \n",
    "    print (\"#################################\")\n",
    "    print (\"DICTIONARY y_data\")\n",
    "    dictt=tokenizer_y.get_config()\n",
    "    print (dictt)\n",
    "    num_words = len(tokenizer_y.word_index) + 1\n",
    "\n",
    "    print (\"################## max token: \",num_words )\n",
    "\n",
    "    #revere\n",
    "    print (\"TEST REVERSE: \")\n",
    "    print (\"y data shape: \", y_data.shape)\n",
    "    y_data_reversed=tokenizer_y.sequences_to_texts (y_data)\n",
    "    \n",
    "    for iii in range (len(y_data_reversed)):\n",
    "        y_data_reversed[iii]=y_data_reversed[iii].upper().strip().replace(\" \", \"\")\n",
    "        \n",
    "    print (\"Element 0\", y_data_reversed[0])\n",
    "    \n",
    "    print (\"Number of y samples\",len (y_data_reversed) )\n",
    "    print (\"Original: \", y_data[:3,:])\n",
    "\n",
    "    print (\"REVERSED TEXT 0..2: \", y_data_reversed[0:3])\n",
    "\n",
    "    print (\"Len 0 as example: \", len (y_data_reversed[0]) )\n",
    "    print (\"Len 2 as example: \", len (y_data_reversed[2]) )\n",
    "    \n",
    "    if maxdata<y_data.shape[0]:\n",
    "        print ('select subset...', maxdata )\n",
    "        X=X[:maxdata]\n",
    "        y_data=y_data[:maxdata]\n",
    "        print (\"new shapes: \", X.shape, y_data.shape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,  y_data , test_size=split,random_state=235)\n",
    "        \n",
    "    train_dataset = RegressionDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float()/ynormfac) #/ynormfac)\n",
    "    \n",
    "    fig_handle = sns.distplot(torch.from_numpy(y_train)*ynormfac,bins=25,kde=False, rug=False,norm_hist=False,axlabel='y labels')\n",
    "    fig = fig_handle.get_figure()\n",
    "    plt.show()\n",
    "    \n",
    "    test_dataset = RegressionDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float()/ynormfac)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size_, shuffle=True)\n",
    "    train_loader_noshuffle = DataLoader(dataset=train_dataset, batch_size=batch_size_, shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size_)\n",
    "    \n",
    "    return train_loader, train_loader_noshuffle, test_loader, tokenizer_y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b8f4270",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'PROTEIN_Mar18_2022_SECSTR_ALL.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m99999999999999999\u001b[39m\n\u001b[1;32m      5\u001b[0m min_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m train_loader, train_loader_noshuffle, test_loader,tokenizer_y \\\n\u001b[0;32m----> 7\u001b[0m         \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_set_seq2seq\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPROTEIN_Mar18_2022_SECSTR_ALL.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmaxdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumber\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m, in \u001b[0;36mload_data_set_seq2seq\u001b[0;34m(file_path, min_length, max_length, batch_size_, output_dim, maxdata, tokenizer_y, split)\u001b[0m\n\u001b[1;32m     17\u001b[0m min_length_measured\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     18\u001b[0m max_length_measured\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m999999\u001b[39m\n\u001b[0;32m---> 19\u001b[0m protein_df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m protein_df\n\u001b[1;32m     22\u001b[0m protein_df\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m~/anaconda3/envs/diff/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diff/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/diff/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diff/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/diff/lib/python3.9/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PROTEIN_Mar18_2022_SECSTR_ALL.csv'"
     ]
    }
   ],
   "source": [
    "ynormfac=22.\n",
    "batch_size_=512\n",
    "max_length = 64\n",
    "number = 99999999999999999\n",
    "min_length=0\n",
    "train_loader, train_loader_noshuffle, test_loader,tokenizer_y \\\n",
    "        = load_data_set_seq2seq (file_path='PROTEIN_Mar18_2022_SECSTR_ALL.csv', \n",
    "                   min_length=0, max_length=max_length, batch_size_=batch_size_, output_dim=3,\n",
    "                  maxdata=number,   split=0.1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27657c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len (train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d303683",
   "metadata": {},
   "source": [
    "### Build diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed4963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f39ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from random import random\n",
    "from typing import List, Union\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial, wraps\n",
    "from contextlib import contextmanager, nullcontext\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch import nn, einsum\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.special import expm1\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import kornia.augmentation as K\n",
    "\n",
    "from einops import rearrange, repeat, reduce\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops_exts import rearrange_many, repeat_many, check_shape\n",
    "from einops_exts.torch import EinopsToAndFrom\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def identity(t, *args, **kwargs):\n",
    "    return t\n",
    "\n",
    "def first(arr, d = None):\n",
    "    if len(arr) == 0:\n",
    "        return d\n",
    "    return arr[0]\n",
    "\n",
    "def maybe(fn):\n",
    "    @wraps(fn)\n",
    "    def inner(x):\n",
    "        if not exists(x):\n",
    "            return x\n",
    "        return fn(x)\n",
    "    return inner\n",
    "\n",
    "def once(fn):\n",
    "    called = False\n",
    "    @wraps(fn)\n",
    "    def inner(x):\n",
    "        nonlocal called\n",
    "        if called:\n",
    "            return\n",
    "        called = True\n",
    "        return fn(x)\n",
    "    return inner\n",
    "\n",
    "print_once = once(print)\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def cast_tuple(val, length = None):\n",
    "    if isinstance(val, list):\n",
    "        val = tuple(val)\n",
    "\n",
    "    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n",
    "\n",
    "    if exists(length):\n",
    "        assert len(output) == length\n",
    "\n",
    "    return output\n",
    "\n",
    "def is_float_dtype(dtype):\n",
    "    return any([dtype == float_dtype for float_dtype in (torch.float64, torch.float32, torch.float16, torch.bfloat16)])\n",
    "\n",
    "def cast_uint8_images_to_float(images):\n",
    "    if not images.dtype == torch.uint8:\n",
    "        return images\n",
    "    return images / 255\n",
    "\n",
    "def module_device(module):\n",
    "    return next(module.parameters()).device\n",
    "\n",
    "def zero_init_(m):\n",
    "    nn.init.zeros_(m.weight)\n",
    "    if exists(m.bias):\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "def eval_decorator(fn):\n",
    "    def inner(model, *args, **kwargs):\n",
    "        was_training = model.training\n",
    "        model.eval()\n",
    "        out = fn(model, *args, **kwargs)\n",
    "        model.train(was_training)\n",
    "        return out\n",
    "    return inner\n",
    "\n",
    "def pad_tuple_to_length(t, length, fillvalue = None):\n",
    "    remain_length = length - len(t)\n",
    "    if remain_length <= 0:\n",
    "        return t\n",
    "    return (*t, *((fillvalue,) * remain_length))\n",
    "\n",
    "# helper classes\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x\n",
    "\n",
    "# tensor helpers\n",
    "\n",
    "def log(t, eps: float = 1e-12):\n",
    "    return torch.log(t.clamp(min = eps))\n",
    "\n",
    "def l2norm(t):\n",
    "    return F.normalize(t, dim = -1)\n",
    "\n",
    "def right_pad_dims_to(x, t):\n",
    "    padding_dims = x.ndim - t.ndim\n",
    "    if padding_dims <= 0:\n",
    "        return t\n",
    "    return t.view(*t.shape, *((1,) * padding_dims))\n",
    "\n",
    "def masked_mean(t, *, dim, mask = None):\n",
    "    if not exists(mask):\n",
    "        return t.mean(dim = dim)\n",
    "\n",
    "    denom = mask.sum(dim = dim, keepdim = True)\n",
    "    mask = rearrange(mask, 'b n -> b n 1')\n",
    "    masked_t = t.masked_fill(~mask, 0.)\n",
    "\n",
    "    return masked_t.sum(dim = dim) / denom.clamp(min = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa90b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_to(\n",
    "    image,\n",
    "    target_image_size,\n",
    "    clamp_range = None\n",
    "):\n",
    "    orig_image_size = image.shape[-1]\n",
    "\n",
    "    if orig_image_size == target_image_size:\n",
    "        return image\n",
    "\n",
    "    out = F.interpolate(image.float(), target_image_size, mode = 'linear', align_corners = True) \n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image normalization functions\n",
    "# ddpms expect images to be in the range of -1 to 1\n",
    "def normalize_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def unnormalize_zero_to_one(normed_img):\n",
    "    return (normed_img + 1) * 0.5\n",
    "\n",
    "# classifier free guidance functions\n",
    "\n",
    "def prob_mask_like(shape, prob, device):\n",
    "    if prob == 1:\n",
    "        return torch.ones(shape, device = device, dtype = torch.bool)\n",
    "    elif prob == 0:\n",
    "        return torch.zeros(shape, device = device, dtype = torch.bool)\n",
    "    else:\n",
    "        return torch.zeros(shape, device = device).float().uniform_(0, 1) < prob\n",
    "\n",
    "# gaussian diffusion with continuous time helper functions and classes\n",
    "# large part of this was thanks to @crowsonkb at https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\n",
    "\n",
    "@torch.jit.script\n",
    "def beta_linear_log_snr(t):\n",
    "    return -torch.log(expm1(1e-4 + 10 * (t ** 2)))\n",
    "\n",
    "@torch.jit.script\n",
    "def alpha_cosine_log_snr(t, s: float = 0.008):\n",
    "    return -log((torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** -2) - 1, eps = 1e-5) # not sure if this accounts for beta being clipped to 0.999 in discrete version\n",
    "\n",
    "def log_snr_to_alpha_sigma(log_snr):\n",
    "    return torch.sqrt(torch.sigmoid(log_snr)), torch.sqrt(torch.sigmoid(-log_snr))\n",
    "\n",
    "class GaussianDiffusionContinuousTimes(nn.Module):\n",
    "    def __init__(self, *, noise_schedule, timesteps = 1000):\n",
    "        super().__init__()\n",
    "\n",
    "        if noise_schedule == \"linear\":\n",
    "            self.log_snr = beta_linear_log_snr\n",
    "        elif noise_schedule == \"cosine\":\n",
    "            self.log_snr = alpha_cosine_log_snr\n",
    "        else:\n",
    "            raise ValueError(f'invalid noise schedule {noise_schedule}')\n",
    "\n",
    "        self.num_timesteps = timesteps\n",
    "\n",
    "    def get_times(self, batch_size, noise_level, *, device):\n",
    "        return torch.full((batch_size,), noise_level, device = device, dtype = torch.float32)\n",
    "\n",
    "    def sample_random_times(self, batch_size, max_thres = 0.999, *, device):\n",
    "        return torch.zeros((batch_size,), device = device).float().uniform_(0, max_thres)\n",
    "\n",
    "    def get_condition(self, times):\n",
    "        return maybe(self.log_snr)(times)\n",
    "\n",
    "    def get_sampling_timesteps(self, batch, *, device):\n",
    "        times = torch.linspace(1., 0., self.num_timesteps + 1, device = device)\n",
    "        times = repeat(times, 't -> b t', b = batch)\n",
    "        times = torch.stack((times[:, :-1], times[:, 1:]), dim = 0)\n",
    "        times = times.unbind(dim = -1)\n",
    "        return times\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t, *, t_next = None):\n",
    "        t_next = default(t_next, lambda: (t - 1. / self.num_timesteps).clamp(min = 0.))\n",
    "\n",
    "        \"\"\" https://openreview.net/attachment?id=2LdBqxc1Yv&name=supplementary_material \"\"\"\n",
    "        log_snr = self.log_snr(t)\n",
    "        log_snr_next = self.log_snr(t_next)\n",
    "        log_snr, log_snr_next = map(partial(right_pad_dims_to, x_t), (log_snr, log_snr_next))\n",
    "\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
    "        alpha_next, sigma_next = log_snr_to_alpha_sigma(log_snr_next)\n",
    "\n",
    "        # c - as defined near eq 33\n",
    "        c = -expm1(log_snr - log_snr_next)\n",
    "        posterior_mean = alpha_next * (x_t * (1 - c) / alpha + c * x_start)\n",
    "\n",
    "        # following (eq. 33)\n",
    "        posterior_variance = (sigma_next ** 2) * c\n",
    "        posterior_log_variance_clipped = log(posterior_variance, eps = 1e-20)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def q_sample(self, x_start, t, noise = None):\n",
    "        dtype = x_start.dtype\n",
    "\n",
    "        if isinstance(t, float):\n",
    "            batch = x_start.shape[0]\n",
    "            t = torch.full((batch,), t, device = x_start.device, dtype = dtype)\n",
    "\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        log_snr = self.log_snr(t).type(dtype)\n",
    "        log_snr_padded_dim = right_pad_dims_to(x_start, log_snr)\n",
    "        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n",
    "\n",
    "        return alpha * x_start + sigma * noise, log_snr\n",
    "\n",
    "    def q_sample_from_to(self, x_from, from_t, to_t, noise = None):\n",
    "        shape, device, dtype = x_from.shape, x_from.device, x_from.dtype\n",
    "        batch = shape[0]\n",
    "\n",
    "        if isinstance(from_t, float):\n",
    "            from_t = torch.full((batch,), from_t, device = device, dtype = dtype)\n",
    "\n",
    "        if isinstance(to_t, float):\n",
    "            to_t = torch.full((batch,), to_t, device = device, dtype = dtype)\n",
    "\n",
    "        noise = default(noise, lambda: torch.randn_like(x_from))\n",
    "\n",
    "        log_snr = self.log_snr(from_t)\n",
    "        log_snr_padded_dim = right_pad_dims_to(x_from, log_snr)\n",
    "        alpha, sigma =  log_snr_to_alpha_sigma(log_snr_padded_dim)\n",
    "\n",
    "        log_snr_to = self.log_snr(to_t)\n",
    "        log_snr_padded_dim_to = right_pad_dims_to(x_from, log_snr_to)\n",
    "        alpha_to, sigma_to =  log_snr_to_alpha_sigma(log_snr_padded_dim_to)\n",
    "\n",
    "        return x_from * (alpha_to / alpha) + noise * (sigma_to * alpha - sigma * alpha_to) / alpha\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        log_snr = self.log_snr(t)\n",
    "        log_snr = right_pad_dims_to(x_t, log_snr)\n",
    "        alpha, sigma = log_snr_to_alpha_sigma(log_snr)\n",
    "        return (x_t - sigma * noise) / alpha.clamp(min = 1e-8)\n",
    "\n",
    "# norms and residuals\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, feats, stable = False, dim = -1):\n",
    "        super().__init__()\n",
    "        self.stable = stable\n",
    "        self.dim = dim\n",
    "\n",
    "        self.g = nn.Parameter(torch.ones(feats, *((1,) * (-dim - 1))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        dtype, dim = x.dtype, self.dim\n",
    "\n",
    "        if self.stable:\n",
    "            x = x / x.amax(dim = dim, keepdim = True).detach()\n",
    "\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "        var = torch.var(x, dim = dim, unbiased = False, keepdim = True)\n",
    "        mean = torch.mean(x, dim = dim, keepdim = True)\n",
    "\n",
    "        return (x - mean) * (var + eps).rsqrt().type(dtype) * self.g.type(dtype)\n",
    "\n",
    "ChanLayerNorm = partial(LayerNorm, dim = -2)\n",
    "\n",
    "class Always():\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.val\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class Parallel(nn.Module):\n",
    "    def __init__(self, *fns):\n",
    "        super().__init__()\n",
    "        self.fns = nn.ModuleList(fns)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [fn(x) for fn in self.fns]\n",
    "        return sum(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention pooling\n",
    "\n",
    "class PerceiverAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        cosine_sim_attn = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5 if not cosine_sim_attn else 1\n",
    "        self.cosine_sim_attn = cosine_sim_attn\n",
    "        self.cosine_sim_scale = 16 if cosine_sim_attn else 1\n",
    "\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_latents = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim, bias = False),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, latents, mask = None):\n",
    "        x = self.norm(x)\n",
    "        latents = self.norm_latents(latents)\n",
    "\n",
    "        b, h = x.shape[0], self.heads\n",
    "\n",
    "        q = self.to_q(latents)\n",
    "\n",
    "        # the paper differs from Perceiver in which they also concat the key / values \n",
    "        #derived from the latents to be attended to\n",
    "        kv_input = torch.cat((x, latents), dim = -2)\n",
    "        k, v = self.to_kv(kv_input).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = h)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        # cosine sim attention\n",
    "\n",
    "        if self.cosine_sim_attn:\n",
    "            q, k = map(l2norm, (q, k))\n",
    "\n",
    "        # similarities and masking\n",
    "\n",
    "        sim = einsum('... i d, ... j d  -> ... i j', q, k) * self.cosine_sim_scale\n",
    "\n",
    "        if exists(mask):\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = F.pad(mask, (0, latents.shape[-2]), value = True)\n",
    "           \n",
    "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
    "            sim = sim.masked_fill(~mask, max_neg_value)\n",
    "\n",
    "        # attention\n",
    "\n",
    "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
    "        attn = attn.to(sim.dtype)\n",
    "\n",
    "        out = einsum('... i j, ... j d -> ... i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        depth,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        num_latents = 64,\n",
    "        num_latents_mean_pooled = 4, # number of latents derived from mean pooled representation of the sequence\n",
    "        max_seq_len = 512,\n",
    "        ff_mult = 4,\n",
    "        cosine_sim_attn = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Embedding(max_seq_len, dim)\n",
    "\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
    "\n",
    "        self.to_latents_from_mean_pooled_seq = None\n",
    "\n",
    "        if num_latents_mean_pooled > 0:\n",
    "            self.to_latents_from_mean_pooled_seq = nn.Sequential(\n",
    "                LayerNorm(dim),\n",
    "                nn.Linear(dim, dim * num_latents_mean_pooled),\n",
    "                Rearrange('b (n d) -> b n d', n = num_latents_mean_pooled)\n",
    "            )\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PerceiverAttention(dim = dim, dim_head = dim_head, heads = heads, cosine_sim_attn = cosine_sim_attn),\n",
    "                FeedForward(dim = dim, mult = ff_mult)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        n, device = x.shape[1], x.device\n",
    "        pos_emb = self.pos_emb(torch.arange(n, device = device))\n",
    "\n",
    "        x_with_pos = x + pos_emb\n",
    "\n",
    "        latents = repeat(self.latents, 'n d -> b n d', b = x.shape[0])\n",
    "\n",
    "        if exists(self.to_latents_from_mean_pooled_seq):\n",
    "            meanpooled_seq = masked_mean(x, dim = 1, mask = torch.ones(x.shape[:2], device = x.device, dtype = torch.bool))\n",
    "            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n",
    "            latents = torch.cat((meanpooled_latents, latents), dim = -2)\n",
    "\n",
    "        for attn, ff in self.layers:\n",
    "            latents = attn(x_with_pos, latents, mask = mask) + latents\n",
    "            latents = ff(latents) + latents\n",
    "\n",
    "        return latents\n",
    "\n",
    "# attention\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        context_dim = None,\n",
    "        cosine_sim_attn = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5 if not cosine_sim_attn else 1.\n",
    "        self.cosine_sim_attn = cosine_sim_attn\n",
    "        self.cosine_sim_scale = 16 if cosine_sim_attn else 1\n",
    "\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, dim_head * 2, bias = False)\n",
    "\n",
    "        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, dim_head * 2)) if exists(context_dim) else None\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim, bias = False),\n",
    "            LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, attn_bias = None):\n",
    "        b, n, device = *x.shape[:2], x.device\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = -1))\n",
    "\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n",
    "        q = q * self.scale\n",
    "\n",
    "        # add null key / value for classifier free guidance in prior net\n",
    "\n",
    "        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b 1 d', b = b)\n",
    "        k = torch.cat((nk, k), dim = -2)\n",
    "        v = torch.cat((nv, v), dim = -2)\n",
    "\n",
    "        # add text conditioning, if present\n",
    "\n",
    "        if exists(context):\n",
    "            assert exists(self.to_context)\n",
    "            ck, cv = self.to_context(context).chunk(2, dim = -1)\n",
    "            k = torch.cat((ck, k), dim = -2)\n",
    "            v = torch.cat((cv, v), dim = -2)\n",
    "\n",
    "        # cosine sim attention\n",
    "\n",
    "        if self.cosine_sim_attn:\n",
    "            q, k = map(l2norm, (q, k))\n",
    "\n",
    "        # calculate query / key similarities\n",
    "\n",
    "        sim = einsum('b h i d, b j d -> b h i j', q, k) * self.cosine_sim_scale\n",
    "\n",
    "        # relative positional encoding (T5 style)\n",
    "\n",
    "        if exists(attn_bias):\n",
    "            sim = sim + attn_bias\n",
    "\n",
    "        # masking\n",
    "\n",
    "        max_neg_value = -torch.finfo(sim.dtype).max\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = F.pad(mask, (1, 0), value = True)\n",
    "\n",
    "            mask = rearrange(mask, 'b j -> b 1 j')\n",
    "            sim = sim.masked_fill(~mask, max_neg_value)\n",
    "\n",
    "        # attention\n",
    "\n",
    "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
    "        attn = attn.to(sim.dtype)\n",
    "\n",
    "        # aggregate values\n",
    "\n",
    "        out = einsum('b h i j, b j d -> b h i d', attn, v)\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "# decoder\n",
    "\n",
    "def Upsample(dim, dim_out = None):\n",
    "    dim_out = default(dim_out, dim)\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n",
    "        nn.Conv1d(dim, dim_out, 3, padding = 1)\n",
    "    )\n",
    "\n",
    "class PixelShuffleUpsample(nn.Module):\n",
    "    \"\"\"\n",
    "    code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts\n",
    "    https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, dim_out = None):\n",
    "        super().__init__()\n",
    "        dim_out = default(dim_out, dim)\n",
    "        conv = nn.Conv1d(dim, dim_out * 4, 1)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            conv,\n",
    "            nn.SiLU(),\n",
    "            nn.PixelShuffle(2)\n",
    "        )\n",
    "\n",
    "        self.init_conv_(conv)\n",
    "\n",
    "    def init_conv_(self, conv):\n",
    "        \n",
    "        o, i, h  = conv.weight.shape\n",
    "        conv_weight = torch.empty(o // 4, i, h )\n",
    "        nn.init.kaiming_uniform_(conv_weight)\n",
    "        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n",
    "\n",
    "        conv.weight.data.copy_(conv_weight)\n",
    "        nn.init.zeros_(conv.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def Downsample(dim, dim_out = None):\n",
    "    # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample\n",
    "    # named SP-conv in the paper, but basically a pixel unshuffle\n",
    "    \n",
    "    dim_out = default(dim_out, dim)\n",
    "   \n",
    "    return nn.Sequential(\n",
    "       \n",
    "        Rearrange('b c (h s1)  -> b (c s1) h', s1 = 2),\n",
    "        nn.Conv1d(dim * 2, dim_out, 1)\n",
    "        \n",
    "    )\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device = x.device) * -emb)\n",
    "        emb = rearrange(x, 'i -> i 1') * rearrange(emb, 'j -> 1 j')\n",
    "        return torch.cat((emb.sin(), emb.cos()), dim = -1)\n",
    "\n",
    "class LearnedSinusoidalPosEmb(nn.Module):\n",
    "    \"\"\" following @crowsonkb 's lead with learned sinusoidal pos emb \"\"\"\n",
    "    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        assert (dim % 2) == 0\n",
    "        half_dim = dim // 2\n",
    "        self.weights = nn.Parameter(torch.randn(half_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b -> b 1')\n",
    "        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n",
    "        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n",
    "        fouriered = torch.cat((x, fouriered), dim = -1)\n",
    "        return fouriered\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_out,\n",
    "        groups = 8,\n",
    "        norm = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.groupnorm = nn.GroupNorm(groups, dim) if norm else Identity()\n",
    "        self.activation = nn.SiLU()\n",
    "        self.project = nn.Conv1d(dim, dim_out, 3, padding = 1)\n",
    "\n",
    "    def forward(self, x, scale_shift = None):\n",
    "        x = self.groupnorm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.activation(x)\n",
    "        return self.project(x)\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_out,\n",
    "        *,\n",
    "        cond_dim = None,\n",
    "        time_cond_dim = None,\n",
    "        groups = 8,\n",
    "        linear_attn = False,\n",
    "        use_gca = False,\n",
    "        squeeze_excite = False,\n",
    "        **attn_kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_mlp = None\n",
    "\n",
    "        if exists(time_cond_dim):\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(time_cond_dim, dim_out * 2)\n",
    "            )\n",
    "\n",
    "        self.cross_attn = None\n",
    "\n",
    "        if exists(cond_dim):\n",
    "            attn_klass = CrossAttention if not linear_attn else LinearCrossAttention\n",
    "\n",
    "            self.cross_attn = EinopsToAndFrom(\n",
    "          \n",
    "                'b c h ',\n",
    "                'b h c',\n",
    "                attn_klass(\n",
    "                    dim = dim_out,\n",
    "                    context_dim = cond_dim,\n",
    "                    **attn_kwargs\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups = groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups = groups)\n",
    "\n",
    "        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out) if use_gca else Always(1)\n",
    "\n",
    "        self.res_conv = nn.Conv1d(dim, dim_out, 1) if dim != dim_out else Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x, time_emb = None, cond = None):\n",
    "\n",
    "        scale_shift = None\n",
    "        if exists(self.time_mlp) and exists(time_emb):\n",
    "            time_emb = self.time_mlp(time_emb)\n",
    "           \n",
    "            time_emb = rearrange(time_emb, 'b c -> b c 1')\n",
    "            scale_shift = time_emb.chunk(2, dim = 1)\n",
    "\n",
    "        h = self.block1(x)\n",
    "\n",
    "        if exists(self.cross_attn):\n",
    "            assert exists(cond)\n",
    "            h = self.cross_attn(h, context = cond) + h\n",
    "\n",
    "        h = self.block2(h, scale_shift = scale_shift)\n",
    "\n",
    "        h = h * self.gca(h)\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        context_dim = None,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        norm_context = False,\n",
    "        cosine_sim_attn = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5 if not cosine_sim_attn else 1.\n",
    "        self.cosine_sim_attn = cosine_sim_attn\n",
    "        self.cosine_sim_scale = 16 if cosine_sim_attn else 1\n",
    "\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        context_dim = default(context_dim, dim)\n",
    "\n",
    "        self.norm = LayerNorm(dim)\n",
    "        self.norm_context = LayerNorm(context_dim) if norm_context else Identity()\n",
    "\n",
    "        self.null_kv = nn.Parameter(torch.randn(2, dim_head))\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim, bias = False),\n",
    "            LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context, mask = None):\n",
    "        b, n, device = *x.shape[:2], x.device\n",
    "\n",
    "        x = self.norm(x)\n",
    "        context = self.norm_context(context)\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "\n",
    "        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h = self.heads)\n",
    "\n",
    "        # add null key / value for classifier free guidance in prior net\n",
    "\n",
    "        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> b h 1 d', h = self.heads,  b = b)\n",
    "\n",
    "        k = torch.cat((nk, k), dim = -2)\n",
    "        v = torch.cat((nv, v), dim = -2)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        # cosine sim attention\n",
    "\n",
    "        if self.cosine_sim_attn:\n",
    "            q, k = map(l2norm, (q, k))\n",
    "\n",
    "        # similarities\n",
    "\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.cosine_sim_scale\n",
    "\n",
    "        # masking\n",
    "\n",
    "        max_neg_value = -torch.finfo(sim.dtype).max\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = F.pad(mask, (1, 0), value = True)\n",
    "         \n",
    "            mask = rearrange(mask, 'b j -> b 1 j')\n",
    "            sim = sim.masked_fill(~mask, max_neg_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1, dtype = torch.float32)\n",
    "        attn = attn.to(sim.dtype)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class LinearCrossAttention(CrossAttention):\n",
    "    def forward(self, x, context, mask = None):\n",
    "        b, n, device = *x.shape[:2], x.device\n",
    "\n",
    "        x = self.norm(x)\n",
    "        context = self.norm_context(context)\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "\n",
    "        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> (b h) n d', h = self.heads)\n",
    "\n",
    "        # add null key / value for classifier free guidance in prior net\n",
    "\n",
    "        nk, nv = repeat_many(self.null_kv.unbind(dim = -2), 'd -> (b h) 1 d', h = self.heads,  b = b)\n",
    "\n",
    "        k = torch.cat((nk, k), dim = -2)\n",
    "        v = torch.cat((nv, v), dim = -2)\n",
    "\n",
    "        # masking\n",
    "\n",
    "        max_neg_value = -torch.finfo(x.dtype).max\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = F.pad(mask, (1, 0), value = True)\n",
    "            mask = rearrange(mask, 'b n -> b n 1')\n",
    "            k = k.masked_fill(~mask, max_neg_value)\n",
    "            v = v.masked_fill(~mask, 0.)\n",
    "\n",
    "        # linear attention\n",
    "\n",
    "        q = q.softmax(dim = -1)\n",
    "        k = k.softmax(dim = -2)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        context = einsum('b n d, b n e -> b d e', k, v)\n",
    "        out = einsum('b n d, b d e -> b n e', q, context)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = self.heads)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_head = 32,\n",
    "        heads = 8,\n",
    "        dropout = 0.05,\n",
    "        context_dim = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        inner_dim = dim_head * heads\n",
    "        self.norm = ChanLayerNorm(dim)\n",
    "\n",
    "        self.nonlin = nn.SiLU()\n",
    "\n",
    "        self.to_q = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(dim, inner_dim, 1, bias = False),\n",
    "            nn.Conv1d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
    "        )\n",
    "\n",
    "        self.to_k = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(dim, inner_dim, 1, bias = False),\n",
    "            nn.Conv1d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
    "        )\n",
    "\n",
    "        self.to_v = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(dim, inner_dim, 1, bias = False),\n",
    "            nn.Conv1d(inner_dim, inner_dim, 3, bias = False, padding = 1, groups = inner_dim)\n",
    "        )\n",
    "\n",
    "        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, inner_dim * 2, bias = False)) if exists(context_dim) else None\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv1d(inner_dim, dim, 1, bias = False),\n",
    "            ChanLayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, fmap, context = None):\n",
    "        h, x, y = self.heads, *fmap.shape[-2:]\n",
    "\n",
    "        fmap = self.norm(fmap)\n",
    "        q, k, v = map(lambda fn: fn(fmap), (self.to_q, self.to_k, self.to_v))\n",
    "        q, k, v = rearrange_many((q, k, v), 'b (h c) x y -> (b h) (x y) c', h = h)\n",
    "\n",
    "        if exists(context):\n",
    "            assert exists(self.to_context)\n",
    "            ck, cv = self.to_context(context).chunk(2, dim = -1)\n",
    "            ck, cv = rearrange_many((ck, cv), 'b n (h d) -> (b h) n d', h = h)\n",
    "            k = torch.cat((k, ck), dim = -2)\n",
    "            v = torch.cat((v, cv), dim = -2)\n",
    "\n",
    "        q = q.softmax(dim = -1)\n",
    "        k = k.softmax(dim = -2)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        context = einsum('b n d, b n e -> b d e', k, v)\n",
    "        out = einsum('b n d, b d e -> b n e', q, context)\n",
    "        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, x = x, y = y)\n",
    "\n",
    "        out = self.nonlin(out)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class GlobalContext(nn.Module):\n",
    "    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim_in,\n",
    "        dim_out\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.to_k = nn.Conv1d(dim_in, 1, 1)\n",
    "        hidden_dim = max(3, dim_out // 2)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(dim_in, hidden_dim, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv1d(hidden_dim, dim_out, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = self.to_k(x)\n",
    "        x, context = rearrange_many((x, context), 'b n ... -> b n (...)')\n",
    "        out = einsum('b i n, b c n -> b c i', context.softmax(dim = -1), x)\n",
    "    \n",
    "        return self.net(out)\n",
    "\n",
    "def FeedForward(dim, mult = 2):\n",
    "    hidden_dim = int(dim * mult)\n",
    "    return nn.Sequential(\n",
    "        LayerNorm(dim),\n",
    "        nn.Linear(dim, hidden_dim, bias = False),\n",
    "        nn.GELU(),\n",
    "        LayerNorm(hidden_dim),\n",
    "        nn.Linear(hidden_dim, dim, bias = False)\n",
    "    )\n",
    "\n",
    "def ChanFeedForward(dim, mult = 2):  # in paper, it seems for self attention layers they did feedforwards with twice channel width\n",
    "    hidden_dim = int(dim * mult)\n",
    "    return nn.Sequential(\n",
    "        ChanLayerNorm(dim),\n",
    "        nn.Conv1d(dim, hidden_dim, 1, bias = False),\n",
    "        nn.GELU(),\n",
    "        ChanLayerNorm(hidden_dim),\n",
    "        nn.Conv1d(hidden_dim, dim, 1, bias = False)\n",
    "    )\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth = 1,\n",
    "        heads = 8,\n",
    "        dim_head = 32,\n",
    "        ff_mult = 2,\n",
    "        context_dim = None,\n",
    "        cosine_sim_attn = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                EinopsToAndFrom('b c h', 'b h c', Attention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim, cosine_sim_attn = cosine_sim_attn)),\n",
    "                ChanFeedForward(dim = dim, mult = ff_mult)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, context = None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, context = context) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class LinearAttentionTransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth = 1,\n",
    "        heads = 8,\n",
    "        dim_head = 32,\n",
    "        ff_mult = 2,\n",
    "        context_dim = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                LinearAttention(dim = dim, heads = heads, dim_head = dim_head, context_dim = context_dim),\n",
    "                ChanFeedForward(dim = dim, mult = ff_mult)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, context = None):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x, context = context) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class CrossEmbedLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        kernel_sizes,\n",
    "        dim_out = None,\n",
    "        stride = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n",
    "        dim_out = default(dim_out, dim_in)\n",
    "\n",
    "        kernel_sizes = sorted(kernel_sizes)\n",
    "        num_scales = len(kernel_sizes)\n",
    "\n",
    "        # calculate the dimension at each scale\n",
    "        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, num_scales)]\n",
    "        dim_scales = [*dim_scales, dim_out - sum(dim_scales)]\n",
    "\n",
    "        self.convs = nn.ModuleList([])\n",
    "        for kernel, dim_scale in zip(kernel_sizes, dim_scales):\n",
    "            self.convs.append(nn.Conv1d(dim_in, dim_scale, kernel, stride = stride, padding = (kernel - stride) // 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmaps = tuple(map(lambda conv: conv(x), self.convs))\n",
    "        return torch.cat(fmaps, dim = 1)\n",
    "\n",
    "class UpsampleCombiner(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        enabled = False,\n",
    "        dim_ins = tuple(),\n",
    "        dim_outs = tuple()\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_outs = cast_tuple(dim_outs, len(dim_ins))\n",
    "        assert len(dim_ins) == len(dim_outs)\n",
    "\n",
    "        self.enabled = enabled\n",
    "\n",
    "        if not self.enabled:\n",
    "            self.dim_out = dim\n",
    "            return\n",
    "\n",
    "        self.fmap_convs = nn.ModuleList([Block(dim_in, dim_out) for dim_in, dim_out in zip(dim_ins, dim_outs)])\n",
    "        self.dim_out = dim + (sum(dim_outs) if len(dim_outs) > 0 else 0)\n",
    "\n",
    "    def forward(self, x, fmaps = None):\n",
    "        target_size = x.shape[-1]\n",
    "\n",
    "        fmaps = default(fmaps, tuple())\n",
    "\n",
    "        if not self.enabled or len(fmaps) == 0 or len(self.fmap_convs) == 0:\n",
    "            return x\n",
    "\n",
    "        fmaps = [resize_image_to(fmap, target_size) for fmap in fmaps]\n",
    "        outs = [conv(fmap) for fmap, conv in zip(fmaps, self.fmap_convs)]\n",
    "        return torch.cat((x, *outs), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac31c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.lowres_cond = False\n",
    "        self.dummy_parameter = nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "    def cast_model_parameters(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "## One-D Unet\n",
    "########################################################\n",
    "class OneD_Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        image_embed_dim = 1024,\n",
    "        text_embed_dim = 768,\n",
    "        num_resnet_blocks = 1,\n",
    "        cond_dim = None,\n",
    "        num_image_tokens = 4,\n",
    "        num_time_tokens = 2,\n",
    "        learned_sinu_pos_emb_dim = 16,\n",
    "        out_dim = None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        cond_images_channels = 0,\n",
    "        channels = 3,\n",
    "        channels_out = None,\n",
    "        attn_dim_head = 64,\n",
    "        attn_heads = 8,\n",
    "        ff_mult = 2.,\n",
    "        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
    "        layer_attns = True,\n",
    "        layer_attns_depth = 1,\n",
    "        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n",
    "        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
    "        layer_cross_attns = True,\n",
    "        use_linear_attn = False,\n",
    "        use_linear_cross_attn = False,\n",
    "        cond_on_text = True,\n",
    "        max_text_len = 256,\n",
    "        init_dim = None,\n",
    "        resnet_groups = 8,\n",
    "        init_conv_kernel_size = 7,          # kernel size of initial conv, if not using cross embed\n",
    "        init_cross_embed = False,\n",
    "        init_cross_embed_kernel_sizes = (3, 7, 15),\n",
    "        cross_embed_downsample = False,\n",
    "        cross_embed_downsample_kernel_sizes = (2, 4),\n",
    "        attn_pool_text = True,\n",
    "        attn_pool_num_latents = 32,\n",
    "        dropout = 0.,\n",
    "        memory_efficient = False,\n",
    "        init_conv_to_final_conv_residual = False,\n",
    "        use_global_context_attn = True,\n",
    "        scale_skip_connection = True,\n",
    "        final_resnet_block = True,\n",
    "        final_conv_kernel_size = 3,\n",
    "        cosine_sim_attn = False,\n",
    "        self_cond = False,\n",
    "        combine_upsample_fmaps = False,      # combine feature maps from all upsample blocks, used in unet squared successfully\n",
    "        pixel_shuffle_upsample = False  ,      # may address checkboard artifacts\n",
    "        beginning_and_final_conv_present = True , #TODO add cross-attn, doesnt work yet...whether or not to have final conv layer\n",
    "           \n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert attn_heads > 1, 'you need to have more than 1 attention head, ideally at least 4 or 8'\n",
    "\n",
    "        if dim < 128:\n",
    "            print_once('The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/')\n",
    "\n",
    "        # save locals to take care of some hyperparameters for cascading DDPM\n",
    "\n",
    "        self._locals = locals()\n",
    "        self._locals.pop('self', None)\n",
    "        self._locals.pop('__class__', None)\n",
    "\n",
    "        # determine dimensions\n",
    "\n",
    "        self.channels = channels\n",
    "        self.channels_out = default(channels_out, channels)\n",
    "\n",
    "        # (1) in cascading diffusion, one concats the low resolution image, blurred, for conditioning the higher resolution synthesis\n",
    "        # (2) in self conditioning, one appends the predict x0 (x_start)\n",
    "        init_channels = channels * (1 + int(lowres_cond) + int(self_cond))\n",
    "        init_dim = default(init_dim, dim)\n",
    "\n",
    "        self.self_cond = self_cond\n",
    "\n",
    "        # optional image conditioning\n",
    "\n",
    "        self.has_cond_image = cond_images_channels > 0\n",
    "        self.cond_images_channels = cond_images_channels\n",
    "\n",
    "        init_channels += cond_images_channels\n",
    "\n",
    "        \n",
    "        self.beginning_and_final_conv_present=beginning_and_final_conv_present\n",
    "        \n",
    "       \n",
    "        if self.beginning_and_final_conv_present:\n",
    "             #self.init_conv=nn.Conv1d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)\n",
    "             self.init_conv = CrossEmbedLayer(init_channels, dim_out = init_dim, \n",
    "                                         kernel_sizes = init_cross_embed_kernel_sizes, stride = 1) if init_cross_embed else nn.Conv1d(init_channels, init_dim, init_conv_kernel_size, padding = init_conv_kernel_size // 2)\n",
    "       \n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        # time conditioning\n",
    "\n",
    "        cond_dim = default(cond_dim, dim)\n",
    "        time_cond_dim = dim * 4 * (2 if lowres_cond else 1)\n",
    "\n",
    "        # embedding time for log(snr) noise from continuous version\n",
    "\n",
    "        sinu_pos_emb = LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim)\n",
    "        sinu_pos_emb_input_dim = learned_sinu_pos_emb_dim + 1\n",
    "\n",
    "        self.to_time_hiddens = nn.Sequential(\n",
    "            sinu_pos_emb,\n",
    "            nn.Linear(sinu_pos_emb_input_dim, time_cond_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.to_time_cond = nn.Sequential(\n",
    "            nn.Linear(time_cond_dim, time_cond_dim)\n",
    "        )\n",
    "\n",
    "        # project to time tokens as well as time hiddens\n",
    "\n",
    "        self.to_time_tokens = nn.Sequential(\n",
    "            nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n",
    "            Rearrange('b (r d) -> b r d', r = num_time_tokens)\n",
    "        )\n",
    "\n",
    "        # low res aug noise conditioning\n",
    "\n",
    "        self.lowres_cond = lowres_cond\n",
    "\n",
    "        if lowres_cond:\n",
    "            self.to_lowres_time_hiddens = nn.Sequential(\n",
    "                LearnedSinusoidalPosEmb(learned_sinu_pos_emb_dim),\n",
    "                nn.Linear(learned_sinu_pos_emb_dim + 1, time_cond_dim),\n",
    "                nn.SiLU()\n",
    "            )\n",
    "\n",
    "            self.to_lowres_time_cond = nn.Sequential(\n",
    "                nn.Linear(time_cond_dim, time_cond_dim)\n",
    "            )\n",
    "\n",
    "            self.to_lowres_time_tokens = nn.Sequential(\n",
    "                nn.Linear(time_cond_dim, cond_dim * num_time_tokens),\n",
    "                Rearrange('b (r d) -> b r d', r = num_time_tokens)\n",
    "            )\n",
    "\n",
    "        # normalizations\n",
    "\n",
    "        self.norm_cond = nn.LayerNorm(cond_dim)\n",
    "\n",
    "        # text encoding conditioning (optional)\n",
    "\n",
    "        self.text_to_cond = None\n",
    "\n",
    "        if cond_on_text:  \n",
    "            assert exists(text_embed_dim), 'text_embed_dim must be given to the unet if cond_on_text is True'\n",
    "            if text_embed_dim != cond_dim:\n",
    "                self.text_to_cond = nn.Linear(text_embed_dim, cond_dim)\n",
    "                self.text_cond_linear=True\n",
    "                \n",
    "            else:\n",
    "                print (\"Text conditioning is equal to cond_dim - no linear layer used\")\n",
    "                self.text_cond_linear=False\n",
    "                \n",
    "\n",
    "        # finer control over whether to condition on text encodings\n",
    "\n",
    "        self.cond_on_text = cond_on_text\n",
    "\n",
    "        # attention pooling\n",
    "\n",
    "        self.attn_pool = PerceiverResampler(dim = cond_dim, depth = 2, \n",
    "                                            dim_head = attn_dim_head, heads = attn_heads, \n",
    "                                            num_latents = attn_pool_num_latents, \n",
    "                                            cosine_sim_attn = cosine_sim_attn) if attn_pool_text else None\n",
    "\n",
    "        # for classifier free guidance\n",
    "\n",
    "        self.max_text_len = max_text_len\n",
    "\n",
    "        self.null_text_embed = nn.Parameter(torch.randn(1, max_text_len, cond_dim))\n",
    "        self.null_text_hidden = nn.Parameter(torch.randn(1, time_cond_dim))\n",
    "\n",
    "        # for non-attention based text conditioning at all points in the network where time is also conditioned\n",
    "\n",
    "        self.to_text_non_attn_cond = None\n",
    "\n",
    "        if cond_on_text:\n",
    "            self.to_text_non_attn_cond = nn.Sequential(\n",
    "                nn.LayerNorm(cond_dim),\n",
    "                nn.Linear(cond_dim, time_cond_dim),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(time_cond_dim, time_cond_dim)\n",
    "            )\n",
    "\n",
    "        # attention related params\n",
    "\n",
    "        attn_kwargs = dict(heads = attn_heads, dim_head = attn_dim_head, cosine_sim_attn = cosine_sim_attn)\n",
    "\n",
    "        num_layers = len(in_out)\n",
    "\n",
    "        # resnet block klass\n",
    "\n",
    "        num_resnet_blocks = cast_tuple(num_resnet_blocks, num_layers)\n",
    "        resnet_groups = cast_tuple(resnet_groups, num_layers)\n",
    "\n",
    "        resnet_klass = partial(ResnetBlock, **attn_kwargs)\n",
    "\n",
    "        layer_attns = cast_tuple(layer_attns, num_layers)\n",
    "        layer_attns_depth = cast_tuple(layer_attns_depth, num_layers)\n",
    "        layer_cross_attns = cast_tuple(layer_cross_attns, num_layers)\n",
    "\n",
    "        use_linear_attn = cast_tuple(use_linear_attn, num_layers)\n",
    "        use_linear_cross_attn = cast_tuple(use_linear_cross_attn, num_layers)\n",
    "\n",
    "        assert all([layers == num_layers for layers in list(map(len, (resnet_groups, layer_attns, layer_cross_attns)))])\n",
    "\n",
    "        # downsample klass\n",
    "\n",
    "        downsample_klass = Downsample\n",
    "\n",
    "        if cross_embed_downsample:\n",
    "            downsample_klass = partial(CrossEmbedLayer, kernel_sizes = cross_embed_downsample_kernel_sizes)\n",
    "\n",
    "        # initial resnet block (for memory efficient unet)\n",
    "\n",
    "        self.init_resnet_block = resnet_klass(init_dim, init_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = use_global_context_attn) if memory_efficient else None\n",
    "\n",
    "        # scale for resnet skip connections\n",
    "\n",
    "        self.skip_connect_scale = 1. if not scale_skip_connection else (2 ** -0.5)\n",
    "\n",
    "        # layers\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        layer_params = [num_resnet_blocks, resnet_groups, layer_attns, layer_attns_depth, layer_cross_attns, use_linear_attn, use_linear_cross_attn]\n",
    "        reversed_layer_params = list(map(reversed, layer_params))\n",
    "\n",
    "        # downsampling layers\n",
    "\n",
    "        skip_connect_dims = [] # keep track of skip connection dimensions\n",
    "\n",
    "        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(in_out, *layer_params)):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n",
    "\n",
    "            if layer_attn:\n",
    "                transformer_block_klass = TransformerBlock\n",
    "            elif layer_use_linear_attn:\n",
    "                transformer_block_klass = LinearAttentionTransformerBlock\n",
    "            else:\n",
    "                transformer_block_klass = Identity\n",
    "\n",
    "            current_dim = dim_in\n",
    "\n",
    "            # whether to pre-downsample, from memory efficient unet\n",
    "\n",
    "            pre_downsample = None\n",
    "\n",
    "            if memory_efficient:\n",
    "                pre_downsample = downsample_klass(dim_in, dim_out)\n",
    "                current_dim = dim_out\n",
    "\n",
    "            skip_connect_dims.append(current_dim)\n",
    "\n",
    "            # whether to do post-downsample, for non-memory efficient unet\n",
    "\n",
    "            post_downsample = None\n",
    "            if not memory_efficient:\n",
    "                post_downsample = downsample_klass(current_dim, dim_out) if not is_last else Parallel(nn.Conv1d(dim_in, dim_out, 3, padding = 1), nn.Conv1d(dim_in, dim_out, 1))\n",
    "\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                pre_downsample,\n",
    "                resnet_klass(current_dim, current_dim, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n",
    "                nn.ModuleList([ResnetBlock(current_dim, current_dim, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n",
    "                transformer_block_klass(dim = current_dim, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n",
    "                post_downsample\n",
    "            ]))\n",
    "\n",
    "        # middle layers\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "\n",
    "        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n",
    "        \n",
    "        self.mid_attn = EinopsToAndFrom('b c h', 'b h c', Residual(Attention(mid_dim, **attn_kwargs))) if attend_at_middle else None\n",
    "        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, cond_dim = cond_dim, time_cond_dim = time_cond_dim, groups = resnet_groups[-1])\n",
    "\n",
    "        # upsample klass\n",
    "\n",
    "        upsample_klass = Upsample if not pixel_shuffle_upsample else PixelShuffleUpsample\n",
    "\n",
    "        # upsampling layers\n",
    "\n",
    "        upsample_fmap_dims = []\n",
    "\n",
    "        for ind, ((dim_in, dim_out), layer_num_resnet_blocks, groups, layer_attn, layer_attn_depth, layer_cross_attn, layer_use_linear_attn, layer_use_linear_cross_attn) in enumerate(zip(reversed(in_out), *reversed_layer_params)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            layer_cond_dim = cond_dim if layer_cross_attn or layer_use_linear_cross_attn else None\n",
    "\n",
    "            if layer_attn:\n",
    "                transformer_block_klass = TransformerBlock\n",
    "            elif layer_use_linear_attn:\n",
    "                transformer_block_klass = LinearAttentionTransformerBlock\n",
    "            else:\n",
    "                transformer_block_klass = Identity\n",
    "\n",
    "            skip_connect_dim = skip_connect_dims.pop()\n",
    "\n",
    "            upsample_fmap_dims.append(dim_out)\n",
    "\n",
    "            self.ups.append(nn.ModuleList([\n",
    "                resnet_klass(dim_out + skip_connect_dim, dim_out, cond_dim = layer_cond_dim, linear_attn = layer_use_linear_cross_attn, time_cond_dim = time_cond_dim, groups = groups),\n",
    "                nn.ModuleList([ResnetBlock(dim_out + skip_connect_dim, dim_out, time_cond_dim = time_cond_dim, groups = groups, use_gca = use_global_context_attn) for _ in range(layer_num_resnet_blocks)]),\n",
    "                transformer_block_klass(dim = dim_out, depth = layer_attn_depth, ff_mult = ff_mult, context_dim = cond_dim, **attn_kwargs),\n",
    "                upsample_klass(dim_out, dim_in) if not is_last or memory_efficient else Identity()\n",
    "            ]))\n",
    "\n",
    "        # whether to combine feature maps from all upsample blocks before final resnet block out\n",
    "\n",
    "        self.upsample_combiner = UpsampleCombiner(\n",
    "            dim = dim,\n",
    "            enabled = combine_upsample_fmaps,\n",
    "            dim_ins = upsample_fmap_dims,\n",
    "            dim_outs = dim\n",
    "        )\n",
    "\n",
    "        # whether to do a final residual from initial conv to the final resnet block out\n",
    "\n",
    "        self.init_conv_to_final_conv_residual = init_conv_to_final_conv_residual\n",
    "        final_conv_dim = self.upsample_combiner.dim_out + (dim if init_conv_to_final_conv_residual else 0)\n",
    "\n",
    "        # final optional resnet block and convolution out\n",
    "\n",
    "        self.final_res_block = ResnetBlock(final_conv_dim, dim, time_cond_dim = time_cond_dim, groups = resnet_groups[0], use_gca = True) if final_resnet_block else None\n",
    "\n",
    "        final_conv_dim_in = dim if final_resnet_block else final_conv_dim\n",
    "        final_conv_dim_in += (channels if lowres_cond else 0)\n",
    "\n",
    "        if self.beginning_and_final_conv_present:\n",
    "            print (final_conv_dim_in, self.channels_out)\n",
    "            self.final_conv = nn.Conv1d(final_conv_dim_in, self.channels_out, final_conv_kernel_size, padding = final_conv_kernel_size // 2)\n",
    "\n",
    "        if self.beginning_and_final_conv_present:\n",
    "            zero_init_(self.final_conv)\n",
    "\n",
    "    # if the current settings for the unet are not correct\n",
    "    # for cascading DDPM, then reinit the unet with the right settings\n",
    "    def cast_model_parameters(\n",
    "        self,\n",
    "        *,\n",
    "        lowres_cond,\n",
    "        text_embed_dim,\n",
    "        channels,\n",
    "        channels_out,\n",
    "        cond_on_text\n",
    "    ):\n",
    "        if lowres_cond == self.lowres_cond and \\\n",
    "            channels == self.channels and \\\n",
    "            cond_on_text == self.cond_on_text and \\\n",
    "            text_embed_dim == self._locals['text_embed_dim'] and \\\n",
    "            channels_out == self.channels_out:\n",
    "            return self\n",
    "\n",
    "        updated_kwargs = dict(\n",
    "            lowres_cond = lowres_cond,\n",
    "            text_embed_dim = text_embed_dim,\n",
    "            channels = channels,\n",
    "            channels_out = channels_out,\n",
    "            cond_on_text = cond_on_text\n",
    "        )\n",
    "\n",
    "        return self.__class__(**{**self._locals, **updated_kwargs})\n",
    "\n",
    "    # methods for returning the full unet config as well as its parameter state\n",
    "\n",
    "    def to_config_and_state_dict(self):\n",
    "        return self._locals, self.state_dict()\n",
    "\n",
    "    # class method for rehydrating the unet from its config and state dict\n",
    "\n",
    "    @classmethod\n",
    "    def from_config_and_state_dict(klass, config, state_dict):\n",
    "        unet = klass(**config)\n",
    "        unet.load_state_dict(state_dict)\n",
    "        return unet\n",
    "\n",
    "    # methods for persisting unet to disk\n",
    "\n",
    "    def persist_to_file(self, path):\n",
    "        path = Path(path)\n",
    "        path.parents[0].mkdir(exist_ok = True, parents = True)\n",
    "\n",
    "        config, state_dict = self.to_config_and_state_dict()\n",
    "        pkg = dict(config = config, state_dict = state_dict)\n",
    "        torch.save(pkg, str(path))\n",
    "\n",
    "    # class method for rehydrating the unet from file saved with `persist_to_file`\n",
    "\n",
    "    @classmethod\n",
    "    def hydrate_from_file(klass, path):\n",
    "        path = Path(path)\n",
    "        assert path.exists()\n",
    "        pkg = torch.load(str(path))\n",
    "\n",
    "        assert 'config' in pkg and 'state_dict' in pkg\n",
    "        config, state_dict = pkg['config'], pkg['state_dict']\n",
    "\n",
    "        return Unet.from_config_and_state_dict(config, state_dict)\n",
    "\n",
    "    # forward with classifier free guidance\n",
    "\n",
    "    def forward_with_cond_scale(\n",
    "        self,\n",
    "        *args,\n",
    "        cond_scale = 1.,\n",
    "        **kwargs\n",
    "    ):\n",
    "        logits = self.forward(*args, **kwargs)\n",
    "\n",
    "        if cond_scale == 1:\n",
    "            return logits\n",
    "\n",
    "        null_logits = self.forward(*args, cond_drop_prob = 1., **kwargs)\n",
    "        return null_logits + (logits - null_logits) * cond_scale\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        time,\n",
    "        *,\n",
    "        lowres_cond_img = None,\n",
    "        lowres_noise_times = None,\n",
    "        text_embeds = None,\n",
    "        text_mask = None,\n",
    "        cond_images = None,\n",
    "        self_cond = None,\n",
    "        cond_drop_prob = 0.\n",
    "    ):\n",
    "        batch_size, device = x.shape[0], x.device\n",
    "\n",
    "        # condition on self\n",
    "\n",
    "        if self.self_cond:\n",
    "            self_cond = default(self_cond, lambda: torch.zeros_like(x))\n",
    "            x = torch.cat((x, self_cond), dim = 1)\n",
    "\n",
    "        # add low resolution conditioning, if present\n",
    "\n",
    "        assert not (self.lowres_cond and not exists(lowres_cond_img)), 'low resolution conditioning image must be present'\n",
    "        assert not (self.lowres_cond and not exists(lowres_noise_times)), 'low resolution conditioning noise time must be present'\n",
    "\n",
    "        if exists(lowres_cond_img):\n",
    "            x = torch.cat((x, lowres_cond_img), dim = 1)\n",
    "\n",
    "        # condition on input image\n",
    "\n",
    "        assert not (self.has_cond_image ^ exists(cond_images)), 'you either requested to condition on an image on the unet, but the conditioning image is not supplied, or vice versa'\n",
    "\n",
    "        if exists(cond_images):\n",
    "            assert cond_images.shape[1] == self.cond_images_channels, 'the number of channels on the conditioning image you are passing in does not match what you specified on initialiation of the unet'\n",
    "            cond_images = resize_image_to(cond_images, x.shape[-1])\n",
    "            x = torch.cat((cond_images, x), dim = 1)\n",
    "\n",
    "        # initial convolution\n",
    "        \n",
    "        #print (\"x input \", x.shape)\n",
    "        if self.beginning_and_final_conv_present:\n",
    "            x = self.init_conv(x)    \n",
    "\n",
    "         \n",
    "        if self.init_conv_to_final_conv_residual:\n",
    "            init_conv_residual = x.clone()\n",
    "\n",
    "        # time conditioning\n",
    "\n",
    "        time_hiddens = self.to_time_hiddens(time)\n",
    "\n",
    "        # derive time tokens\n",
    "\n",
    "        time_tokens = self.to_time_tokens(time_hiddens)\n",
    "        t = self.to_time_cond(time_hiddens)\n",
    "\n",
    "        # add lowres time conditioning to time hiddens\n",
    "        # and add lowres time tokens along sequence dimension for attention\n",
    "\n",
    "        if self.lowres_cond:\n",
    "            lowres_time_hiddens = self.to_lowres_time_hiddens(lowres_noise_times)\n",
    "            lowres_time_tokens = self.to_lowres_time_tokens(lowres_time_hiddens)\n",
    "            lowres_t = self.to_lowres_time_cond(lowres_time_hiddens)\n",
    "\n",
    "            t = t + lowres_t\n",
    "           \n",
    "            time_tokens = torch.cat((time_tokens, lowres_time_tokens), dim = -2)\n",
    "\n",
    "        # text conditioning\n",
    "\n",
    "        text_tokens = None\n",
    "\n",
    "    \n",
    "        if exists(text_embeds) and self.cond_on_text:\n",
    "\n",
    "            # conditional dropout\n",
    "\n",
    "            text_keep_mask = prob_mask_like((batch_size,), 1 - cond_drop_prob, device = device)\n",
    "\n",
    "           \n",
    "            text_keep_mask_embed = rearrange(text_keep_mask, 'b -> b 1 1')\n",
    "            text_keep_mask_hidden = rearrange(text_keep_mask, 'b -> b 1')\n",
    "\n",
    "            # calculate text embeds\n",
    "            \n",
    "            if self.text_cond_linear:\n",
    "                text_tokens = self.text_to_cond(text_embeds)\n",
    "            else:\n",
    "                text_tokens=text_embeds\n",
    "\n",
    "            text_tokens = text_tokens[:, :self.max_text_len]\n",
    "           \n",
    "            if exists(text_mask):\n",
    "                text_mask = text_mask[:, :self.max_text_len]\n",
    "\n",
    "            text_tokens_len = text_tokens.shape[1]\n",
    "            remainder = self.max_text_len - text_tokens_len\n",
    "            \n",
    "            if remainder > 0:\n",
    "                 \n",
    "                text_tokens = F.pad(text_tokens, (0, 0, 0, remainder))\n",
    "\n",
    "            if exists(text_mask):\n",
    "                if remainder > 0:\n",
    "                    text_mask = F.pad(text_mask, (0, remainder), value = False)\n",
    "\n",
    "                \n",
    "                text_mask = rearrange(text_mask, 'b n -> b n 1')\n",
    "                text_keep_mask_embed = text_mask & text_keep_mask_embed\n",
    "            \n",
    "            null_text_embed = self.null_text_embed.to(text_tokens.dtype) # for some reason pytorch AMP not working\n",
    "           \n",
    "            text_tokens = torch.where(\n",
    "                text_keep_mask_embed,\n",
    "                text_tokens,\n",
    "                null_text_embed\n",
    "            )\n",
    "            \n",
    "            if exists(self.attn_pool):\n",
    "                text_tokens = self.attn_pool(text_tokens)\n",
    "\n",
    "            # extra non-attention conditioning by projecting and then summing text embeddings to time\n",
    "            # termed as text hiddens\n",
    "\n",
    "             \n",
    "            mean_pooled_text_tokens = text_tokens.mean(dim = -2)\n",
    "\n",
    "            text_hiddens = self.to_text_non_attn_cond(mean_pooled_text_tokens)\n",
    "\n",
    "            null_text_hidden = self.null_text_hidden.to(t.dtype)\n",
    "\n",
    "            text_hiddens = torch.where(\n",
    "                text_keep_mask_hidden,\n",
    "                text_hiddens,\n",
    "                null_text_hidden\n",
    "            )\n",
    "\n",
    "            t = t + text_hiddens\n",
    "\n",
    "        # main conditioning tokens (c)\n",
    "     \n",
    "    \n",
    "        c = time_tokens if not exists(text_tokens) else torch.cat((time_tokens, text_tokens), dim = -2)\n",
    "        \n",
    "        # normalize conditioning tokens\n",
    "\n",
    "        c = self.norm_cond(c)\n",
    "\n",
    "        \n",
    "        # initial resnet block (for memory efficient unet)\n",
    "\n",
    "        if exists(self.init_resnet_block):\n",
    "            x = self.init_resnet_block(x, t)\n",
    "\n",
    "    \n",
    "        # go through the layers of the unet, down and up\n",
    "\n",
    "        hiddens = []\n",
    "\n",
    "        for pre_downsample, init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n",
    "            if exists(pre_downsample):\n",
    "                x = pre_downsample(x)\n",
    "            x = init_block(x, t, c)\n",
    "            for resnet_block in resnet_blocks:\n",
    "                x = resnet_block(x, t)\n",
    "                hiddens.append(x)\n",
    "            \n",
    "            x = attn_block(x, c)\n",
    "            \n",
    "            hiddens.append(x)\n",
    "\n",
    "            if exists(post_downsample):\n",
    "                x = post_downsample(x)\n",
    "                \n",
    "\n",
    "        x = self.mid_block1(x, t, c)\n",
    "       \n",
    "        if exists(self.mid_attn):\n",
    "            x = self.mid_attn(x)\n",
    "\n",
    "        x = self.mid_block2(x, t, c)\n",
    "       \n",
    "        add_skip_connection = lambda x: torch.cat((x, hiddens.pop() * self.skip_connect_scale), dim = 1)\n",
    "\n",
    "        up_hiddens = []\n",
    "        \n",
    "        for init_block, resnet_blocks, attn_block, upsample in self.ups:\n",
    "         \n",
    "            x = add_skip_connection(x)\n",
    "           \n",
    "            x = init_block(x, t, c)\n",
    "\n",
    "            for resnet_block in resnet_blocks:\n",
    "                x = add_skip_connection(x)\n",
    "                x = resnet_block(x, t)\n",
    "\n",
    "            x = attn_block(x, c)\n",
    "            up_hiddens.append(x.contiguous())\n",
    "            x = upsample(x)\n",
    "\n",
    "        # whether to combine all feature maps from upsample blocks\n",
    "\n",
    "        x = self.upsample_combiner(x, up_hiddens)\n",
    "\n",
    "        # final top-most residual if needed\n",
    "\n",
    "        if self.init_conv_to_final_conv_residual:\n",
    "            x = torch.cat((x, init_conv_residual), dim = 1)\n",
    "\n",
    "        if exists(self.final_res_block):\n",
    "            x = self.final_res_block(x, t)\n",
    "\n",
    "        if exists(lowres_cond_img):\n",
    "            x = torch.cat((x, lowres_cond_img), dim = 1)\n",
    "            \n",
    "        if self.beginning_and_final_conv_present:\n",
    "            x=self.final_conv(x) \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694caa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null unet\n",
    "\n",
    "class NullUnet(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.lowres_cond = False\n",
    "        self.dummy_parameter = nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "    def cast_model_parameters(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x\n",
    "\n",
    "# predefined unets, with configs lining up with hyperparameters in appendix of paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdcad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "## Elucidated denoising model \n",
    "## After: Tero Karras and Miika Aittala and Timo Aila and Samuli Laine, \n",
    "##        Elucidating the Design Space of Diffusion-Based Generative Models\n",
    "##        https://arxiv.org/abs/2206.00364, 2022\n",
    "########################################################\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "# constants\n",
    "\n",
    "Hparams_fields = [\n",
    "    'num_sample_steps',\n",
    "    'sigma_min',\n",
    "    'sigma_max',\n",
    "    'sigma_data',\n",
    "    'rho',\n",
    "    'P_mean',\n",
    "    'P_std',\n",
    "    'S_churn',\n",
    "    'S_tmin',\n",
    "    'S_tmax',\n",
    "    'S_noise'\n",
    "]\n",
    "\n",
    "Hparams = namedtuple('Hparams', Hparams_fields)\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def log(t, eps = 1e-20):\n",
    "    return torch.log(t.clamp(min = eps))\n",
    "\n",
    "# main class\n",
    "\n",
    "class ElucidatedImagen(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unets,\n",
    "        *,\n",
    "        image_sizes,                                # for cascading ddpm, image size at each stage\n",
    "        text_encoder_name = '',\n",
    "        text_embed_dim = None,\n",
    "        channels = 3,\n",
    "        channels_out=3,\n",
    "        cond_drop_prob = 0.1,\n",
    "        random_crop_sizes = None,\n",
    "        lowres_sample_noise_level = 0.2,            # in the paper, they present a new trick where they noise the lowres conditioning image, and at sample time, fix it to a certain level (0.1 or 0.3) - the unets are also made to be conditioned on this noise level\n",
    "        per_sample_random_aug_noise_level = False,  # unclear when conditioning on augmentation noise level, whether each batch element receives a random aug noise value - turning off due to @marunine's find\n",
    "        condition_on_text = True,\n",
    "        auto_normalize_img = True,                  # whether to take care of normalizing the image from [0, 1] to [-1, 1] and back automatically - you can turn this off if you want to pass in the [-1, 1] ranged image yourself from the dataloader\n",
    "        dynamic_thresholding = True,\n",
    "        dynamic_thresholding_percentile = 0.95,     # unsure what this was based on perusal of paper\n",
    "        only_train_unet_number = None,\n",
    "        lowres_noise_schedule = 'linear',\n",
    "        num_sample_steps = 32,                      # number of sampling steps\n",
    "        sigma_min = 0.002,                          # min noise level\n",
    "        sigma_max = 80,                             # max noise level\n",
    "        sigma_data = 0.5,                           # standard deviation of data distribution\n",
    "        rho = 7,                                    # controls the sampling schedule\n",
    "        P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n",
    "        P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n",
    "        S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n",
    "        S_tmin = 0.05,\n",
    "        S_tmax = 50,\n",
    "        S_noise = 1.003,\n",
    "        #categorical_loss = False,\n",
    "        loss_type=0, #0=MSE, 1=Cross entropy, 2=KLDIv Loss\n",
    "        categorical_loss_ignore=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.only_train_unet_number = only_train_unet_number\n",
    "\n",
    "        # conditioning hparams\n",
    "\n",
    "        self.condition_on_text = condition_on_text\n",
    "        self.unconditional = not condition_on_text\n",
    "        self.loss_type=loss_type\n",
    "        if self.loss_type>0:\n",
    "            self.categorical_loss=True\n",
    "            self.m = nn.LogSoftmax(dim=1) #used for some loss functins\n",
    "        else:\n",
    "            self.categorical_loss=False\n",
    "            \n",
    "        print (\"Loss type: \", self.loss_type)\n",
    "        self.categorical_loss_ignore=categorical_loss_ignore\n",
    "        \n",
    "        # channels\n",
    "\n",
    "        self.channels = channels\n",
    "        self.channels_out = channels_out\n",
    "        \n",
    "\n",
    "        # automatically take care of ensuring that first unet is unconditional\n",
    "        # while the rest of the unets are conditioned on the low resolution image produced by previous unet\n",
    "\n",
    "        unets = cast_tuple(unets)\n",
    "        num_unets = len(unets)\n",
    "\n",
    "        # randomly cropping for upsampler training\n",
    "\n",
    "        self.random_crop_sizes = cast_tuple(random_crop_sizes, num_unets)\n",
    "        assert not exists(first(self.random_crop_sizes)), 'you should not need to randomly crop image during training for base unet, only for upsamplers - so pass in `random_crop_sizes = (None, 128, 256)` as example'\n",
    "\n",
    "        # lowres augmentation noise schedule\n",
    "\n",
    "        self.lowres_noise_schedule = GaussianDiffusionContinuousTimes(noise_schedule = lowres_noise_schedule)\n",
    "      \n",
    "        self.text_embed_dim =text_embed_dim\n",
    "        \n",
    "        # construct unets\n",
    "\n",
    "        self.unets = nn.ModuleList([])\n",
    "        self.unet_being_trained_index = -1 # keeps track of which unet is being trained at the moment\n",
    "\n",
    "        print (f\"Channels in={self.channels}, channels out={self.channels_out}\")\n",
    "        for ind, one_unet in enumerate(unets):\n",
    "            \n",
    "            assert isinstance(one_unet, (  OneD_Unet, NullUnet))\n",
    "            is_first = ind == 0\n",
    "\n",
    "            one_unet = one_unet.cast_model_parameters(\n",
    "                lowres_cond = not is_first,\n",
    "                cond_on_text = self.condition_on_text,\n",
    "                text_embed_dim = self.text_embed_dim if self.condition_on_text else None,\n",
    "                channels = self.channels,\n",
    "                #channels_out = self.channels\n",
    "                channels_out = self.channels_out\n",
    "            )\n",
    "\n",
    "            self.unets.append(one_unet)\n",
    "\n",
    "        # determine whether we are training on images or video\n",
    "\n",
    "        is_video = False  \n",
    "        self.is_video = is_video\n",
    "\n",
    "        self.right_pad_dims_to_datatype = partial(rearrange, pattern = ('b -> b 1 1' if not is_video else 'b -> b 1 1 1'))\n",
    "        self.resize_to = resize_video_to if is_video else resize_image_to\n",
    "\n",
    "        # unet image sizes\n",
    "\n",
    "         \n",
    "        self.image_sizes = image_sizes\n",
    "        assert num_unets == len(self.image_sizes), f'you did not supply the correct number of u-nets ({len(self.unets)}) for resolutions {self.image_sizes}'\n",
    "\n",
    "        self.sample_channels = cast_tuple(self.channels, num_unets)\n",
    "\n",
    "        # cascading ddpm related stuff\n",
    "\n",
    "        lowres_conditions = tuple(map(lambda t: t.lowres_cond, self.unets))\n",
    "        assert lowres_conditions == (False, *((True,) * (num_unets - 1))), 'the first unet must be unconditioned (by low resolution image), and the rest of the unets must have `lowres_cond` set to True'\n",
    "\n",
    "        self.lowres_sample_noise_level = lowres_sample_noise_level\n",
    "        self.per_sample_random_aug_noise_level = per_sample_random_aug_noise_level\n",
    "\n",
    "        # classifier free guidance\n",
    "\n",
    "        self.cond_drop_prob = cond_drop_prob\n",
    "        self.can_classifier_guidance = cond_drop_prob > 0.\n",
    "\n",
    "        # normalize and unnormalize image functions\n",
    "\n",
    "        self.normalize_img = normalize_neg_one_to_one if auto_normalize_img else identity\n",
    "        self.unnormalize_img = unnormalize_zero_to_one if auto_normalize_img else identity\n",
    "        self.input_image_range = (0. if auto_normalize_img else -1., 1.)\n",
    "\n",
    "        # dynamic thresholding\n",
    "\n",
    "        self.dynamic_thresholding = cast_tuple(dynamic_thresholding, num_unets)\n",
    "        self.dynamic_thresholding_percentile = dynamic_thresholding_percentile\n",
    "\n",
    "        # elucidating parameters\n",
    "\n",
    "        hparams = [\n",
    "            num_sample_steps,\n",
    "            sigma_min,\n",
    "            sigma_max,\n",
    "            sigma_data,\n",
    "            rho,\n",
    "            P_mean,\n",
    "            P_std,\n",
    "            S_churn,\n",
    "            S_tmin,\n",
    "            S_tmax,\n",
    "            S_noise,\n",
    "        ]\n",
    "\n",
    "        hparams = [cast_tuple(hp, num_unets) for hp in hparams]\n",
    "        self.hparams = [Hparams(*unet_hp) for unet_hp in zip(*hparams)]\n",
    "\n",
    "        # one temp parameter for keeping track of device\n",
    "\n",
    "        self.register_buffer('_temp', torch.tensor([0.]), persistent = False)\n",
    "\n",
    "        # default to device of unets passed in\n",
    "\n",
    "        self.to(next(self.unets.parameters()).device)\n",
    "\n",
    "    def force_unconditional_(self):\n",
    "        self.condition_on_text = False\n",
    "        self.unconditional = True\n",
    "\n",
    "        for unet in self.unets:\n",
    "            unet.cond_on_text = False\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._temp.device\n",
    "\n",
    "    def get_unet(self, unet_number):\n",
    "        assert 0 < unet_number <= len(self.unets)\n",
    "        index = unet_number - 1\n",
    "\n",
    "        if isinstance(self.unets, nn.ModuleList):\n",
    "            unets_list = [unet for unet in self.unets]\n",
    "            delattr(self, 'unets')\n",
    "            self.unets = unets_list\n",
    "\n",
    "        if index != self.unet_being_trained_index:\n",
    "            for unet_index, unet in enumerate(self.unets):\n",
    "                unet.to(self.device if unet_index == index else 'cpu')\n",
    "\n",
    "        self.unet_being_trained_index = index\n",
    "        return self.unets[index]\n",
    "\n",
    "    def reset_unets_all_one_device(self, device = None):\n",
    "        device = default(device, self.device)\n",
    "        self.unets = nn.ModuleList([*self.unets])\n",
    "        self.unets.to(device)\n",
    "\n",
    "        self.unet_being_trained_index = -1\n",
    "\n",
    "    @contextmanager\n",
    "    def one_unet_in_gpu(self, unet_number = None, unet = None):\n",
    "        assert exists(unet_number) ^ exists(unet)\n",
    "\n",
    "        if exists(unet_number):\n",
    "            unet = self.unets[unet_number - 1]\n",
    "\n",
    "        devices = [module_device(unet) for unet in self.unets]\n",
    "        self.unets.cpu()\n",
    "        unet.to(self.device)\n",
    "\n",
    "        yield\n",
    "\n",
    "        for unet, device in zip(self.unets, devices):\n",
    "            unet.to(device)\n",
    "\n",
    "    # overriding state dict functions\n",
    "\n",
    "    def state_dict(self, *args, **kwargs):\n",
    "        self.reset_unets_all_one_device()\n",
    "        return super().state_dict(*args, **kwargs)\n",
    "\n",
    "    def load_state_dict(self, *args, **kwargs):\n",
    "        self.reset_unets_all_one_device()\n",
    "        return super().load_state_dict(*args, **kwargs)\n",
    "\n",
    "    # dynamic thresholding\n",
    "\n",
    "    def threshold_x_start(self, x_start, dynamic_threshold = True):\n",
    "        if not dynamic_threshold:\n",
    "            return x_start.clamp(-1., 1.)\n",
    "\n",
    "        s = torch.quantile(\n",
    "            rearrange(x_start, 'b ... -> b (...)').abs(),\n",
    "            self.dynamic_thresholding_percentile,\n",
    "            dim = -1\n",
    "        )\n",
    "\n",
    "        s.clamp_(min = 1.)\n",
    "        s = right_pad_dims_to(x_start, s)\n",
    "        return x_start.clamp(-s, s) / s\n",
    "\n",
    "    # derived preconditioning params - Table 1\n",
    "\n",
    "    def c_skip(self, sigma_data, sigma):\n",
    "        return (sigma_data ** 2) / (sigma ** 2 + sigma_data ** 2)\n",
    "\n",
    "    def c_out(self, sigma_data, sigma):\n",
    "        return sigma * sigma_data * (sigma_data ** 2 + sigma ** 2) ** -0.5\n",
    "\n",
    "    def c_in(self, sigma_data, sigma):\n",
    "        return 1 * (sigma ** 2 + sigma_data ** 2) ** -0.5\n",
    "\n",
    "    def c_noise(self, sigma):\n",
    "        return log(sigma) * 0.25\n",
    "\n",
    "   # preconditioned network output\n",
    "    # equation (7) in the paper\n",
    "\n",
    "    def preconditioned_network_forward(\n",
    "        self,\n",
    "        unet_forward,\n",
    "        noised_images,\n",
    "        sigma,\n",
    "        *,\n",
    "        sigma_data,\n",
    "        clamp = False,\n",
    "        dynamic_threshold = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        batch, device = noised_images.shape[0], noised_images.device\n",
    "\n",
    "        if isinstance(sigma, float):\n",
    "            sigma = torch.full((batch,), sigma, device = device)\n",
    "\n",
    "        padded_sigma = self.right_pad_dims_to_datatype(sigma)\n",
    "\n",
    "        net_out = unet_forward(\n",
    "            self.c_in(sigma_data, padded_sigma) * noised_images,\n",
    "            self.c_noise(sigma),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        out = self.c_skip(sigma_data, padded_sigma) * noised_images +  self.c_out(sigma_data, padded_sigma) * net_out\n",
    "\n",
    "        if not clamp:\n",
    "            return out\n",
    "\n",
    "        return self.threshold_x_start(out, dynamic_threshold)\n",
    "\n",
    "    # sampling\n",
    "\n",
    "    # sample schedule\n",
    "    # equation (5) in the paper\n",
    "\n",
    "    def sample_schedule(\n",
    "        self,\n",
    "        num_sample_steps,\n",
    "        rho,\n",
    "        sigma_min,\n",
    "        sigma_max\n",
    "    ):\n",
    "        N = num_sample_steps\n",
    "        inv_rho = 1 / rho\n",
    "\n",
    "        steps = torch.arange(num_sample_steps, device = self.device, dtype = torch.float32)\n",
    "        sigmas = (sigma_max ** inv_rho + steps / (N - 1) * (sigma_min ** inv_rho - sigma_max ** inv_rho)) ** rho\n",
    "\n",
    "        sigmas = F.pad(sigmas, (0, 1), value = 0.) # last step is sigma value of 0.\n",
    "        return sigmas\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def one_unet_sample(\n",
    "        self,\n",
    "        unet,\n",
    "        shape,\n",
    "        *,\n",
    "        unet_number,\n",
    "        clamp = True,\n",
    "        dynamic_threshold = True,\n",
    "        cond_scale = 1.,\n",
    "        use_tqdm = True,\n",
    "        inpaint_images = None,\n",
    "        inpaint_masks = None,\n",
    "        inpaint_resample_times = 5,\n",
    "        init_images = None,\n",
    "        skip_steps = None,\n",
    "        sigma_min = None,\n",
    "        sigma_max = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # get specific sampling hyperparameters for unet\n",
    "\n",
    "        hp = self.hparams[unet_number - 1]\n",
    "\n",
    "        sigma_min = default(sigma_min, hp.sigma_min)\n",
    "        sigma_max = default(sigma_max, hp.sigma_max)\n",
    "\n",
    "        # get the schedule, which is returned as (sigma, gamma) tuple, and pair up with the next sigma and gamma\n",
    "\n",
    "        sigmas = self.sample_schedule(hp.num_sample_steps, hp.rho, sigma_min, sigma_max)\n",
    "\n",
    "        gammas = torch.where(\n",
    "            (sigmas >= hp.S_tmin) & (sigmas <= hp.S_tmax),\n",
    "            min(hp.S_churn / hp.num_sample_steps, sqrt(2) - 1),\n",
    "            0.\n",
    "        )\n",
    "\n",
    "        sigmas_and_gammas = list(zip(sigmas[:-1], sigmas[1:], gammas[:-1]))\n",
    "\n",
    "        # images is noise at the beginning\n",
    "\n",
    "        init_sigma = sigmas[0]\n",
    "\n",
    "        images = init_sigma * torch.randn(shape, device = self.device)\n",
    "\n",
    "        # initializing with an image\n",
    "\n",
    "        if exists(init_images):\n",
    "            images += init_images\n",
    "\n",
    "        # keeping track of x0, for self conditioning if needed\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        # prepare inpainting images and mask\n",
    "\n",
    "        has_inpainting = exists(inpaint_images) and exists(inpaint_masks)\n",
    "        resample_times = inpaint_resample_times if has_inpainting else 1\n",
    "\n",
    "        if has_inpainting:\n",
    "            inpaint_images = self.normalize_img(inpaint_images)\n",
    "            inpaint_images = self.resize_to(inpaint_images, shape[-1])\n",
    "            inpaint_masks = self.resize_to(rearrange(inpaint_masks, 'b ... -> b 1 ...').float(), shape[-1]).bool()\n",
    "\n",
    "        # unet kwargs\n",
    "\n",
    "        unet_kwargs = dict(\n",
    "            sigma_data = hp.sigma_data,\n",
    "            clamp = clamp,\n",
    "            dynamic_threshold = dynamic_threshold,\n",
    "            cond_scale = cond_scale,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # gradually denoise\n",
    "\n",
    "        initial_step = default(skip_steps, 0)\n",
    "        sigmas_and_gammas = sigmas_and_gammas[initial_step:]\n",
    "\n",
    "        total_steps = len(sigmas_and_gammas)\n",
    "\n",
    "        for ind, (sigma, sigma_next, gamma) in tqdm(enumerate(sigmas_and_gammas), total = total_steps, desc = 'sampling time step', disable = not use_tqdm):\n",
    "            is_last_timestep = ind == (total_steps - 1)\n",
    "\n",
    "            sigma, sigma_next, gamma = map(lambda t: t.item(), (sigma, sigma_next, gamma))\n",
    "\n",
    "            for r in reversed(range(resample_times)):\n",
    "                is_last_resample_step = r == 0\n",
    "\n",
    "                eps = hp.S_noise * torch.randn(shape, device = self.device) # stochastic sampling\n",
    "\n",
    "                sigma_hat = sigma + gamma * sigma\n",
    "                added_noise = sqrt(sigma_hat ** 2 - sigma ** 2) * eps\n",
    "\n",
    "                images_hat = images + added_noise\n",
    "\n",
    "                self_cond = x_start if unet.self_cond else None\n",
    "\n",
    "                if has_inpainting:\n",
    "                    images_hat = images_hat * ~inpaint_masks + (inpaint_images + added_noise) * inpaint_masks\n",
    "\n",
    "                model_output = self.preconditioned_network_forward(\n",
    "                    unet.forward_with_cond_scale,\n",
    "                    images_hat,\n",
    "                    sigma_hat,\n",
    "                    self_cond = self_cond,\n",
    "                    **unet_kwargs\n",
    "                )\n",
    "                \n",
    "                \n",
    "                denoised_over_sigma = (images_hat - model_output) / sigma_hat\n",
    "\n",
    "                images_next = images_hat + (sigma_next - sigma_hat) * denoised_over_sigma\n",
    "\n",
    "                # second order correction, if not the last timestep\n",
    "\n",
    "                if sigma_next != 0:\n",
    "                    self_cond = model_output if unet.self_cond else None\n",
    "\n",
    "                    model_output_next = self.preconditioned_network_forward(\n",
    "                        unet.forward_with_cond_scale,\n",
    "                        images_next,\n",
    "                        sigma_next,\n",
    "                        self_cond = self_cond,\n",
    "                        **unet_kwargs\n",
    "                    )\n",
    "                    \n",
    "                    denoised_prime_over_sigma = (images_next - model_output_next) / sigma_next\n",
    "                    images_next = images_hat + 0.5 * (sigma_next - sigma_hat) * (denoised_over_sigma + denoised_prime_over_sigma)\n",
    "\n",
    "                images = images_next\n",
    "\n",
    "                if has_inpainting and not (is_last_resample_step or is_last_timestep):\n",
    "                    # renoise in repaint and then resample\n",
    "                    repaint_noise = torch.randn(shape, device = self.device)\n",
    "                    images = images + (sigma - sigma_next) * repaint_noise\n",
    "\n",
    "                x_start = model_output  # save model output for self conditioning\n",
    "                \n",
    "\n",
    "        if has_inpainting:\n",
    "            images = images * ~inpaint_masks + inpaint_images * inpaint_masks\n",
    "\n",
    "        return  images\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @eval_decorator\n",
    "    def sample(\n",
    "        self,\n",
    "        texts: List[str] = None,\n",
    "        text_masks = None,\n",
    "        text_embeds = None,\n",
    "        cond_images = None,\n",
    "        inpaint_images = None,\n",
    "        inpaint_masks = None,\n",
    "        inpaint_resample_times = 5,\n",
    "        init_images = None,\n",
    "        skip_steps = None,\n",
    "        sigma_min = None,\n",
    "        sigma_max = None,\n",
    "        video_frames = None,\n",
    "        batch_size = 1,\n",
    "        cond_scale = 1.,\n",
    "        lowres_sample_noise_level = None,\n",
    "        start_at_unet_number = 1,\n",
    "        start_image_or_video = None,\n",
    "        stop_at_unet_number = None,\n",
    "        return_all_unet_outputs = False,\n",
    "        return_pil_images = False,\n",
    "        use_tqdm = True,\n",
    "        device = None,\n",
    "        \n",
    "    ):\n",
    "        device = default(device, self.device)\n",
    "        self.reset_unets_all_one_device(device = device)\n",
    "\n",
    "        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n",
    "\n",
    "        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n",
    "            assert all([*map(len, texts)]), 'text cannot be empty'\n",
    "\n",
    "            with autocast(enabled = False):\n",
    "                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n",
    "\n",
    "            text_embeds, text_masks = map(lambda t: t.to(device), (text_embeds, text_masks))\n",
    "\n",
    "        if not self.unconditional:\n",
    "            assert exists(text_embeds), 'text must be passed in if the network was not trained without text `condition_on_text` must be set to `False` when training'\n",
    "\n",
    "            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n",
    "            batch_size = text_embeds.shape[0]\n",
    "\n",
    "        if exists(inpaint_images):\n",
    "            if self.unconditional:\n",
    "                if batch_size == 1: # assume researcher wants to broadcast along inpainted images\n",
    "                    batch_size = inpaint_images.shape[0]\n",
    "\n",
    "            assert inpaint_images.shape[0] == batch_size, 'number of inpainting images must be equal to the specified batch size on sample `sample(batch_size=<int>)``'\n",
    "            assert not (self.condition_on_text and inpaint_images.shape[0] != text_embeds.shape[0]), 'number of inpainting images must be equal to the number of text to be conditioned on'\n",
    "\n",
    "        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into imagen if specified'\n",
    "        assert not (not self.condition_on_text and exists(text_embeds)), 'imagen specified not to be conditioned on text, yet it is presented'\n",
    "        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n",
    "\n",
    "        assert not (exists(inpaint_images) ^ exists(inpaint_masks)),  'inpaint images and masks must be both passed in to do inpainting'\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        is_cuda = next(self.parameters()).is_cuda\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        lowres_sample_noise_level = default(lowres_sample_noise_level, self.lowres_sample_noise_level)\n",
    "\n",
    "        num_unets = len(self.unets)\n",
    "        cond_scale = cast_tuple(cond_scale, num_unets)\n",
    "\n",
    "        # handle video and frame dimension\n",
    "\n",
    "        assert not (self.is_video and not exists(video_frames)), 'video_frames must be passed in on sample time if training on video'\n",
    "\n",
    "        frame_dims = (video_frames,) if self.is_video else tuple()\n",
    "\n",
    "        # initializing with an image or video\n",
    "\n",
    "        init_images = cast_tuple(init_images, num_unets)\n",
    "        init_images = [maybe(self.normalize_img)(init_image) for init_image in init_images]\n",
    "\n",
    "        skip_steps = cast_tuple(skip_steps, num_unets)\n",
    "\n",
    "        sigma_min = cast_tuple(sigma_min, num_unets)\n",
    "        sigma_max = cast_tuple(sigma_max, num_unets)\n",
    "\n",
    "        # handle starting at a unet greater than 1, for training only-upscaler training\n",
    "\n",
    "        if start_at_unet_number > 1:\n",
    "            assert start_at_unet_number <= num_unets, 'must start a unet that is less than the total number of unets'\n",
    "            assert not exists(stop_at_unet_number) or start_at_unet_number <= stop_at_unet_number\n",
    "            assert exists(start_image_or_video), 'starting image or video must be supplied if only doing upscaling'\n",
    "\n",
    "            prev_image_size = self.image_sizes[start_at_unet_number - 2]\n",
    "            img = self.resize_to(start_image_or_video, prev_image_size)\n",
    "\n",
    "        \n",
    "        for unet_number, unet, channel, image_size, unet_hparam, dynamic_threshold, unet_cond_scale, unet_init_images, unet_skip_steps, unet_sigma_min, unet_sigma_max in tqdm(zip(range(1, num_unets + 1), self.unets, self.sample_channels, self.image_sizes, self.hparams, self.dynamic_thresholding, cond_scale, init_images, skip_steps, sigma_min, sigma_max), disable = not use_tqdm):\n",
    "            if unet_number < start_at_unet_number:\n",
    "                continue\n",
    "\n",
    "            assert not isinstance(unet, NullUnet), 'cannot sample from null unet'\n",
    "\n",
    "            context = self.one_unet_in_gpu(unet = unet) if is_cuda else nullcontext()\n",
    "\n",
    "            with context:\n",
    "                lowres_cond_img = lowres_noise_times = None\n",
    "\n",
    "                shape = (batch_size, channel, *frame_dims, image_size )\n",
    "\n",
    "                if unet.lowres_cond:\n",
    "                    lowres_noise_times = self.lowres_noise_schedule.get_times(batch_size, lowres_sample_noise_level, device = device)\n",
    "\n",
    "                    lowres_cond_img = self.resize_to(img, image_size)\n",
    "                    \n",
    "                   \n",
    "                    lowres_cond_img = self.normalize_img(lowres_cond_img.float())\n",
    "\n",
    "                    lowres_cond_img, _ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img.float(), \n",
    "                                                                             t = lowres_noise_times, \n",
    "                                                                             noise = torch.randn_like(lowres_cond_img.float()))\n",
    "\n",
    "                if exists(unet_init_images):\n",
    "                    unet_init_images = self.resize_to(unet_init_images, image_size)\n",
    "\n",
    "                #shape = (batch_size, self.channels, *frame_dims, image_size, image_size)\n",
    "                shape = (batch_size, self.channels, *frame_dims, image_size)\n",
    "\n",
    "                img = self.one_unet_sample(\n",
    "                    unet,\n",
    "                    shape,\n",
    "                    unet_number = unet_number,\n",
    "                    text_embeds = text_embeds,\n",
    "                    text_mask =text_masks,\n",
    "                    cond_images = cond_images,\n",
    "                    inpaint_images = inpaint_images,\n",
    "                    inpaint_masks = inpaint_masks,\n",
    "                    inpaint_resample_times = inpaint_resample_times,\n",
    "                    init_images = unet_init_images,\n",
    "                    skip_steps = unet_skip_steps,\n",
    "                    sigma_min = unet_sigma_min,\n",
    "                    sigma_max = unet_sigma_max,\n",
    "                    cond_scale = unet_cond_scale,\n",
    "                    lowres_cond_img = lowres_cond_img,\n",
    "                    lowres_noise_times = lowres_noise_times,\n",
    "                    dynamic_threshold = dynamic_threshold,\n",
    "                    use_tqdm = use_tqdm\n",
    "                )\n",
    " \n",
    "                if self.categorical_loss:\n",
    "                    img=self.m(img)\n",
    "                outputs.append(img)\n",
    "\n",
    "            if exists(stop_at_unet_number) and stop_at_unet_number == unet_number:\n",
    "                break\n",
    "\n",
    "        output_index = -1 if not return_all_unet_outputs else slice(None) # either return last unet output or all unet outputs\n",
    "\n",
    "\n",
    "        if not return_all_unet_outputs:\n",
    "            outputs = outputs[-1:]\n",
    "\n",
    "        assert not self.is_video, 'automatically converting video tensor to video file for saving is not built yet'\n",
    "        \n",
    "         \n",
    "        if self.categorical_loss:\n",
    "            return  torch.argmax(outputs[output_index], dim=1).unsqueeze (1)\n",
    "        else:\n",
    "            return  outputs[output_index]\n",
    "       \n",
    "    # training\n",
    "\n",
    "    def loss_weight(self, sigma_data, sigma):\n",
    "        return (sigma ** 2 + sigma_data ** 2) * (sigma * sigma_data) ** -2\n",
    "\n",
    "    def noise_distribution(self, P_mean, P_std, batch_size):\n",
    "        return (P_mean + P_std * torch.randn((batch_size,), device = self.device)).exp()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images,\n",
    "        unet: Union[OneD_Unet,NullUnet, DistributedDataParallel] = None,\n",
    "        texts: List[str] = None,\n",
    "        text_embeds = None,\n",
    "        text_masks = None,\n",
    "        unet_number = None,\n",
    "        cond_images = None,\n",
    "       \n",
    "    ):\n",
    "        assert not (len(self.unets) > 1 and not exists(unet_number)), f'you must specify which unet you want trained, from a range of 1 to {len(self.unets)}, if you are training cascading DDPM (multiple unets)'\n",
    "        unet_number = default(unet_number, 1)\n",
    "        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you can only train on unet #{self.only_train_unet_number}'\n",
    "\n",
    "       \n",
    "        cond_images = maybe(cast_uint8_images_to_float)(cond_images)\n",
    "        \n",
    "        if self.categorical_loss==False:\n",
    "            assert is_float_dtype(images.dtype), f'images tensor needs to be floats but {images.dtype} dtype found instead'\n",
    "\n",
    "        unet_index = unet_number - 1\n",
    "        \n",
    "        unet = default(unet, lambda: self.get_unet(unet_number))\n",
    "\n",
    "        assert not isinstance(unet, NullUnet), 'null unet cannot and should not be trained'\n",
    "\n",
    "        target_image_size    = self.image_sizes[unet_index]\n",
    "        random_crop_size     = self.random_crop_sizes[unet_index]\n",
    "        prev_image_size      = self.image_sizes[unet_index - 1] if unet_index > 0 else None\n",
    "        hp                   = self.hparams[unet_index]\n",
    "\n",
    "        batch_size, c, *_, h, device, is_video = *images.shape, images.device, (images.ndim == 4)\n",
    "\n",
    "        frames = images.shape[2] if is_video else None\n",
    "\n",
    "        check_shape(images, 'b c ...', c = self.channels)\n",
    "\n",
    "       \n",
    "        assert h >= target_image_size\n",
    "\n",
    "        if exists(texts) and not exists(text_embeds) and not self.unconditional:\n",
    "            assert all([*map(len, texts)]), 'text cannot be empty'\n",
    "            assert len(texts) == len(images), 'number of text captions does not match up with the number of images given'\n",
    "\n",
    "            with autocast(enabled = False):\n",
    "                text_embeds, text_masks = self.encode_text(texts, return_attn_mask = True)\n",
    "\n",
    "            text_embeds, text_masks = map(lambda t: t.to(images.device), (text_embeds, text_masks))\n",
    "\n",
    "        if not self.unconditional:\n",
    "            text_masks = default(text_masks, lambda: torch.any(text_embeds != 0., dim = -1))\n",
    "\n",
    "        assert not (self.condition_on_text and not exists(text_embeds)), 'text or text encodings must be passed into decoder if specified'\n",
    "        assert not (not self.condition_on_text and exists(text_embeds)), 'decoder specified not to be conditioned on text, yet it is presented'\n",
    "\n",
    "        assert not (exists(text_embeds) and text_embeds.shape[-1] != self.text_embed_dim), f'invalid text embedding dimension being passed in (should be {self.text_embed_dim})'\n",
    "\n",
    "        lowres_cond_img = lowres_aug_times = None\n",
    "        if exists(prev_image_size):\n",
    "            lowres_cond_img = self.resize_to(images, prev_image_size, clamp_range = self.input_image_range)\n",
    "            lowres_cond_img = self.resize_to(lowres_cond_img, target_image_size, clamp_range = self.input_image_range)\n",
    "\n",
    "            if self.per_sample_random_aug_noise_level:\n",
    "                lowres_aug_times = self.lowres_noise_schedule.sample_random_times(batch_size, device = device)\n",
    "            else:\n",
    "                lowres_aug_time = self.lowres_noise_schedule.sample_random_times(1, device = device)\n",
    "                lowres_aug_times = repeat(lowres_aug_time, '1 -> b', b = batch_size)\n",
    "\n",
    "    \n",
    "        if exists(random_crop_size):\n",
    "            aug = K.RandomCrop((random_crop_size, random_crop_size), p = 1.)\n",
    "\n",
    "            if is_video:\n",
    "                images, lowres_cond_img = rearrange_many((images, lowres_cond_img), 'b c f h -> (b f) c h')\n",
    "\n",
    "            images = aug(images)\n",
    "            lowres_cond_img = aug(lowres_cond_img, params = aug._params)\n",
    "\n",
    "            if is_video:\n",
    "                images, lowres_cond_img = rearrange_many((images, lowres_cond_img), '(b f) c h -> b c f h', f = frames)\n",
    "\n",
    "        # noise the lowres conditioning image\n",
    "        # at sample time, they then fix the noise level of 0.1 - 0.3\n",
    "\n",
    "        lowres_cond_img_noisy = None\n",
    "        if exists(lowres_cond_img):\n",
    "            lowres_cond_img_noisy, _ = self.lowres_noise_schedule.q_sample(x_start = lowres_cond_img, \n",
    "                                                                           t = lowres_aug_times,\n",
    "                                                                           noise = torch.randn_like(lowres_cond_img.float()))\n",
    "\n",
    "        # get the sigmas\n",
    "\n",
    "        sigmas = self.noise_distribution(hp.P_mean, hp.P_std, batch_size)\n",
    "        padded_sigmas = self.right_pad_dims_to_datatype(sigmas)\n",
    "\n",
    "        # noise\n",
    "\n",
    "        noise = torch.randn_like(images.float())\n",
    "        \n",
    "        noised_images = images + padded_sigmas * noise  # alphas are 1. in the paper\n",
    "     \n",
    "        unet_kwargs = dict(\n",
    "            sigma_data = hp.sigma_data,\n",
    "            text_embeds = text_embeds,\n",
    "            text_mask =text_masks,\n",
    "            cond_images = cond_images,\n",
    "            lowres_noise_times = self.lowres_noise_schedule.get_condition(lowres_aug_times),\n",
    "            lowres_cond_img = lowres_cond_img_noisy,\n",
    "            cond_drop_prob = self.cond_drop_prob,\n",
    "        )\n",
    "\n",
    "        self_cond = unet.module.self_cond if isinstance(unet, DistributedDataParallel) else unet\n",
    "\n",
    "        if self_cond and random() < 0.5:\n",
    "            with torch.no_grad():\n",
    "                pred_x0 = self.preconditioned_network_forward(\n",
    "                    unet.forward,\n",
    "                    noised_images,\n",
    "                    sigmas,\n",
    "                    **unet_kwargs\n",
    "                ).detach()\n",
    "\n",
    "            unet_kwargs = {**unet_kwargs, 'self_cond': pred_x0}\n",
    "\n",
    "        # get prediction\n",
    "\n",
    "        denoised_images = self.preconditioned_network_forward(\n",
    "            unet.forward,\n",
    "            noised_images,\n",
    "            sigmas,\n",
    "            **unet_kwargs\n",
    "        )\n",
    "      \n",
    "        if self.loss_type==0: #self.categorical_loss == False:\n",
    "\n",
    "            losses = F.mse_loss(denoised_images, images, reduction = 'none')\n",
    "            losses = reduce(losses, 'b ... -> b', 'mean')\n",
    "\n",
    "            # loss weighting\n",
    "\n",
    "            losses = losses * self.loss_weight(hp.sigma_data, sigmas)\n",
    "            losses=losses.mean()\n",
    "        \n",
    "       \n",
    "\n",
    "        return losses "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bca487e-4f72-4034-9f69-59a94668370c",
   "metadata": {},
   "source": [
    "## Define protein generative diffusion model - Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a50b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDesigner_A(nn.Module):\n",
    "    def __init__(self, timesteps=10 , dim=32,pred_dim=25,loss_type=0, elucidated=False ,  \n",
    "                  padding_idx=0,\n",
    "                cond_dim = 512,\n",
    "                text_embed_dim = 512,\n",
    "                 input_tokens=25,#for non-BERT\n",
    "                 sequence_embed=False,\n",
    "                 embed_dim_position=32,\n",
    "                 max_text_len=16,\n",
    "                 device='cuda:0',\n",
    "                \n",
    "                ):\n",
    "        super(ProteinDesigner_A, self).__init__()\n",
    "         \n",
    "        self.device=device\n",
    "        self.pred_dim=pred_dim\n",
    "        self.loss_type=loss_type\n",
    "        \n",
    "        self.fc_embed1 = nn.Linear( 8,  max_length)  # NOT USED\n",
    "        self.fc_embed2 = nn.Linear( 1,  text_embed_dim)  # \n",
    "        self.max_text_len=max_text_len\n",
    "       \n",
    "        self.pos_emb_x = nn.Embedding(max_text_len+1, embed_dim_position)\n",
    "        text_embed_dim=text_embed_dim+embed_dim_position\n",
    "        self.pos_matrix_i = torch.zeros (max_text_len, dtype=torch.long)\n",
    "        for i in range (max_text_len):\n",
    "            self.pos_matrix_i [i]=i +1         \n",
    "\n",
    "        assert (loss_type==0), \"Loss other than MSE not implemented\" \n",
    "\n",
    "        unet1 = OneD_Unet(\n",
    "            dim = dim, \n",
    "            text_embed_dim = text_embed_dim,\n",
    "            cond_dim = cond_dim, \n",
    "            dim_mults = (1, 2, 4, 8),\n",
    "           \n",
    "            num_resnet_blocks = 1,#1,\n",
    "            layer_attns = (False, True, True, False),\n",
    "            layer_cross_attns = (False, True, True, False),\n",
    "            channels=self.pred_dim,\n",
    "            channels_out=self.pred_dim ,       \n",
    "         #   \n",
    "        attn_dim_head = 64,\n",
    "        attn_heads = 8,\n",
    "        ff_mult = 2.,\n",
    "        lowres_cond = False,                # for cascading diffusion - https://cascaded-diffusion.github.io/\n",
    "        \n",
    "        layer_attns_depth =1,\n",
    "        layer_attns_add_text_cond = True,   # whether to condition the self-attention blocks with the text embeddings, as described in Appendix D.3.1\n",
    "        attend_at_middle = True,            # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
    "        use_linear_attn = False,\n",
    "        use_linear_cross_attn = False,\n",
    "        cond_on_text = True,\n",
    "        max_text_len = max_length,\n",
    "        init_dim = None,\n",
    "        resnet_groups = 8,\n",
    "        init_conv_kernel_size =7,        # kernel size of initial conv, if not using cross embed\n",
    "        init_cross_embed = False, #TODO - fix ouput size calcs for conv1d\n",
    "        init_cross_embed_kernel_sizes = (3, 7, 15),\n",
    "        cross_embed_downsample = False,\n",
    "        cross_embed_downsample_kernel_sizes = (2, 4),\n",
    "        attn_pool_text = True,\n",
    "        attn_pool_num_latents = 32,#32,   #perceiver model latents \n",
    "        dropout = 0.,\n",
    "        memory_efficient = False,\n",
    "        init_conv_to_final_conv_residual = False,\n",
    "        use_global_context_attn = True,\n",
    "        scale_skip_connection = True,\n",
    "        final_resnet_block = True, \n",
    "        final_conv_kernel_size = 3,\n",
    "        cosine_sim_attn = True,\n",
    "        self_cond = False,\n",
    "        combine_upsample_fmaps = True,      # combine feature maps from all upsample blocks, used in unet squared successfully\n",
    "        pixel_shuffle_upsample = False   ,     # may address checkboard artifacts\n",
    "            \n",
    "        ).to (self.device)\n",
    "      \n",
    "        assert elucidated , \"Only elucidated model implemented....\"\n",
    "        self.is_elucidated=elucidated\n",
    "        if elucidated:\n",
    "            self.imagen = ElucidatedImagen(\n",
    "                unets = (unet1),\n",
    "                channels=self.pred_dim,\n",
    "                channels_out=self.pred_dim ,\n",
    "                loss_type=loss_type, \n",
    "                text_embed_dim = text_embed_dim,\n",
    "                image_sizes = [max_length],\n",
    "                cond_drop_prob = 0.2,\n",
    "                auto_normalize_img = False, \n",
    "                num_sample_steps = timesteps,#(64, 32), # number of sample steps - 64 for base unet, 32 for upsampler (just an example, have no clue what the optimal values are)\n",
    "                sigma_min = 0.002,           # min noise level\n",
    "                sigma_max = 160,#(80, 160),       # max noise level, @crowsonkb recommends double the max noise level for upsampler\n",
    "                sigma_data = 0.5,            # standard deviation of data distribution\n",
    "                rho = 7,                     # controls the sampling schedule\n",
    "                P_mean = -1.2,               # mean of log-normal distribution from which noise is drawn for training\n",
    "                P_std = 1.2,                 # standard deviation of log-normal distribution from which noise is drawn for training\n",
    "                S_churn = 40,#80,                # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n",
    "                S_tmin = 0.05,\n",
    "                S_tmax = 50,\n",
    "                S_noise = 1.003,\n",
    "                 \n",
    "                    ).to (self.device)\n",
    "        else:\n",
    "             print (\"Not implemented.\")\n",
    "            \n",
    "    def forward(self, x, output, unet_number=1): #sequences=conditioning, output=prediction \n",
    "       \n",
    "        x=x.unsqueeze (2)\n",
    "      \n",
    "        x= self.fc_embed2(x)\n",
    "      \n",
    "        pos_matrix_i_=self.pos_matrix_i.repeat(x.shape[0], 1).to (self.device)\n",
    "        pos_emb_x = self.pos_emb_x( pos_matrix_i_)\n",
    "        pos_emb_x = torch.squeeze(pos_emb_x, 1)\n",
    "        pos_emb_x[:,x.shape[1]:,:]=0#set all to zero that are not provided via x\n",
    "        pos_emb_x=pos_emb_x[:,:x.shape[1],:]\n",
    "        x= torch.cat( (x,   pos_emb_x ), 2)        \n",
    "        \n",
    "        loss =  self.imagen(output, text_embeds = x, unet_number = unet_number, )\n",
    " \n",
    "        return loss\n",
    "    \n",
    "    def sample (self, x, stop_at_unet_number=1 ,cond_scale=7.5,):\n",
    "        \n",
    "        x=x.unsqueeze (2)\n",
    "    \n",
    "        x= self.fc_embed2(x)\n",
    "       \n",
    "        pos_matrix_i_=self.pos_matrix_i.repeat(x.shape[0], 1).to (self.device) \n",
    "        pos_emb_x = self.pos_emb_x( pos_matrix_i_)\n",
    "        pos_emb_x = torch.squeeze(pos_emb_x, 1)\n",
    "        pos_emb_x[:,x.shape[1]:,:]=0#set all to zero that are not provided via x\n",
    "        pos_emb_x=pos_emb_x[:,:x.shape[1],:]\n",
    "        x= torch.cat( (x,   pos_emb_x ), 2)    \n",
    "      \n",
    "        output =  self.imagen.sample(text_embeds= x, cond_scale = cond_scale, stop_at_unet_number=stop_at_unet_number)\n",
    "       \n",
    "        return output \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a00e032",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from math import ceil\n",
    "from contextlib import contextmanager, nullcontext\n",
    "from functools import partial, wraps\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import pytorch_warmup as warmup\n",
    "__version__ = '1.9.3'\n",
    "\n",
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "            \n",
    " \n",
    "from packaging import version\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "from accelerate import Accelerator, DistributedType, DistributedDataParallelKwargs\n",
    "\n",
    "from fsspec.core import url_to_fs\n",
    "from fsspec.implementations.local import LocalFileSystem\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def cast_tuple(val, length = 1):\n",
    "    if isinstance(val, list):\n",
    "        val = tuple(val)\n",
    "    \n",
    "    return val if isinstance(val, tuple) else ((val,) * length)\n",
    "\n",
    "def find_first(fn, arr):\n",
    "    for ind, el in enumerate(arr):\n",
    "        if fn(el):\n",
    "            return ind\n",
    "    return -1\n",
    "\n",
    "def pick_and_pop(keys, d):\n",
    "    values = list(map(lambda key: d.pop(key), keys))\n",
    "    return dict(zip(keys, values))\n",
    "\n",
    "def group_dict_by_key(cond, d):\n",
    "    return_val = [dict(),dict()]\n",
    "    for key in d.keys():\n",
    "        match = bool(cond(key))\n",
    "        ind = int(not match)\n",
    "        return_val[ind][key] = d[key]\n",
    "    return (*return_val,)\n",
    "\n",
    "def string_begins_with(prefix, str):\n",
    "    return str.startswith(prefix)\n",
    "\n",
    "def group_by_key_prefix(prefix, d):\n",
    "    return group_dict_by_key(partial(string_begins_with, prefix), d)\n",
    "\n",
    "def groupby_prefix_and_trim(prefix, d):\n",
    "    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n",
    "    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n",
    "    return kwargs_without_prefix, kwargs\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "# url to fs, bucket, path - for checkpointing to cloud\n",
    "\n",
    "def url_to_bucket(url):\n",
    "    if '://' not in url:\n",
    "        return url\n",
    "\n",
    "    _, suffix = url.split('://')\n",
    "\n",
    "    if prefix in {'gs', 's3'}:\n",
    "        return suffix.split('/')[0]\n",
    "    else:\n",
    "        raise ValueError(f'storage type prefix \"{prefix}\" is not supported yet')\n",
    "\n",
    "# decorators\n",
    "\n",
    "def eval_decorator(fn):\n",
    "    def inner(model, *args, **kwargs):\n",
    "        was_training = model.training\n",
    "        model.eval()\n",
    "        out = fn(model, *args, **kwargs)\n",
    "        model.train(was_training)\n",
    "        return out\n",
    "    return inner\n",
    "\n",
    "def cast_torch_tensor(fn, cast_fp16 = False):\n",
    "    @wraps(fn)\n",
    "    def inner(model, *args, **kwargs):\n",
    "        device = kwargs.pop('_device', model.device)\n",
    "        cast_device = kwargs.pop('_cast_device', True)\n",
    "\n",
    "        should_cast_fp16 = cast_fp16 and model.cast_half_at_training\n",
    "\n",
    "        kwargs_keys = kwargs.keys()\n",
    "        all_args = (*args, *kwargs.values())\n",
    "        split_kwargs_index = len(all_args) - len(kwargs_keys)\n",
    "        all_args = tuple(map(lambda t: torch.from_numpy(t) if exists(t) and isinstance(t, np.ndarray) else t, all_args))\n",
    "\n",
    "        if cast_device:\n",
    "            all_args = tuple(map(lambda t: t.to(device) if exists(t) and isinstance(t, torch.Tensor) else t, all_args))\n",
    "\n",
    "        if should_cast_fp16:\n",
    "            all_args = tuple(map(lambda t: t.half() if exists(t) and isinstance(t, torch.Tensor) and t.dtype != torch.bool else t, all_args))\n",
    "\n",
    "        args, kwargs_values = all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n",
    "        kwargs = dict(tuple(zip(kwargs_keys, kwargs_values)))\n",
    "\n",
    "        out = fn(model, *args, **kwargs)\n",
    "        return out\n",
    "    return inner\n",
    "\n",
    "# gradient accumulation functions\n",
    "\n",
    "def split_iterable(it, split_size):\n",
    "    accum = []\n",
    "    for ind in range(ceil(len(it) / split_size)):\n",
    "        start_index = ind * split_size\n",
    "        accum.append(it[start_index: (start_index + split_size)])\n",
    "    return accum\n",
    "\n",
    "def split(t, split_size = None):\n",
    "    if not exists(split_size):\n",
    "        return t\n",
    "\n",
    "    if isinstance(t, torch.Tensor):\n",
    "        return t.split(split_size, dim = 0)\n",
    "\n",
    "    if isinstance(t, Iterable):\n",
    "        return split_iterable(t, split_size)\n",
    "\n",
    "    return TypeError\n",
    "\n",
    "def find_first(cond, arr):\n",
    "    for el in arr:\n",
    "        if cond(el):\n",
    "            return el\n",
    "    return None\n",
    "\n",
    "def split_args_and_kwargs(*args, split_size = None, **kwargs):\n",
    "    all_args = (*args, *kwargs.values())\n",
    "    len_all_args = len(all_args)\n",
    "    first_tensor = find_first(lambda t: isinstance(t, torch.Tensor), all_args)\n",
    "    assert exists(first_tensor)\n",
    "\n",
    "    batch_size = len(first_tensor)\n",
    "    split_size = default(split_size, batch_size)\n",
    "    num_chunks = ceil(batch_size / split_size)\n",
    "\n",
    "    dict_len = len(kwargs)\n",
    "    dict_keys = kwargs.keys()\n",
    "    split_kwargs_index = len_all_args - dict_len\n",
    "\n",
    "    split_all_args = [split(arg, split_size = split_size) if exists(arg) and isinstance(arg, (torch.Tensor, Iterable)) else ((arg,) * num_chunks) for arg in all_args]\n",
    "    chunk_sizes = tuple(map(len, split_all_args[0]))\n",
    "\n",
    "    for (chunk_size, *chunked_all_args) in tuple(zip(chunk_sizes, *split_all_args)):\n",
    "        chunked_args, chunked_kwargs_values = chunked_all_args[:split_kwargs_index], chunked_all_args[split_kwargs_index:]\n",
    "        chunked_kwargs = dict(tuple(zip(dict_keys, chunked_kwargs_values)))\n",
    "        chunk_size_frac = chunk_size / batch_size\n",
    "        yield chunk_size_frac, (chunked_args, chunked_kwargs)\n",
    "\n",
    "# imagen trainer\n",
    "\n",
    "def imagen_sample_in_chunks(fn):\n",
    "    @wraps(fn)\n",
    "    def inner(self, *args, max_batch_size = None, **kwargs):\n",
    "        if not exists(max_batch_size):\n",
    "            return fn(self, *args, **kwargs)\n",
    "\n",
    "        if self.imagen.unconditional:\n",
    "            batch_size = kwargs.get('batch_size')\n",
    "            batch_sizes = num_to_groups(batch_size, max_batch_size)\n",
    "            outputs = [fn(self, *args, **{**kwargs, 'batch_size': sub_batch_size}) for sub_batch_size in batch_sizes]\n",
    "        else:\n",
    "            outputs = [fn(self, *chunked_args, **chunked_kwargs) for _, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs)]\n",
    "\n",
    "        if isinstance(outputs[0], torch.Tensor):\n",
    "            return torch.cat(outputs, dim = 0)\n",
    "\n",
    "        return list(map(lambda t: torch.cat(t, dim = 0), list(zip(*outputs))))\n",
    "\n",
    "    return inner\n",
    "\n",
    "\n",
    "def restore_parts(state_dict_target, state_dict_from):\n",
    "    for name, param in state_dict_from.items():\n",
    "\n",
    "        if name not in state_dict_target:\n",
    "            continue\n",
    "\n",
    "        if param.size() == state_dict_target[name].size():\n",
    "            state_dict_target[name].copy_(param)\n",
    "        else:\n",
    "            print(f\"layer {name}({param.size()} different than target: {state_dict_target[name].size()}\")\n",
    "\n",
    "    return state_dict_target\n",
    "\n",
    "\n",
    "class ImagenTrainer(nn.Module):\n",
    "    locked = False\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        #imagen = None,\n",
    "        model = None,\n",
    "        \n",
    "        imagen_checkpoint_path = None,\n",
    "        use_ema = True,\n",
    "        lr = 1e-4,\n",
    "        eps = 1e-8,\n",
    "        beta1 = 0.9,\n",
    "        beta2 = 0.99,\n",
    "        max_grad_norm = None,\n",
    "        group_wd_params = True,\n",
    "        warmup_steps = None,\n",
    "        cosine_decay_max_steps = None,\n",
    "        only_train_unet_number = None,\n",
    "        fp16 = False,\n",
    "        precision = None,\n",
    "        split_batches = True,\n",
    "        dl_tuple_output_keywords_names = ('images', 'text_embeds', 'text_masks', 'cond_images'),\n",
    "        verbose = True,\n",
    "        split_valid_fraction = 0.025,\n",
    "        split_valid_from_train = False,\n",
    "        split_random_seed = 42,\n",
    "        checkpoint_path = None,\n",
    "        checkpoint_every = None,\n",
    "        checkpoint_fs = None,\n",
    "        fs_kwargs: dict = None,\n",
    "        max_checkpoints_keep = 20,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert not ImagenTrainer.locked, 'ImagenTrainer can only be initialized once per process - for the sake of distributed training, you will now have to create a separate script to train each unet (or a script that accepts unet number as an argument)'\n",
    "\n",
    "        self.fs = checkpoint_fs\n",
    "\n",
    "        if not exists(self.fs):\n",
    "            fs_kwargs = default(fs_kwargs, {})\n",
    "            self.fs, _ = url_to_fs(default(checkpoint_path, './'), **fs_kwargs)\n",
    "        \n",
    "        ema_kwargs, kwargs = groupby_prefix_and_trim('ema_', kwargs)\n",
    "\n",
    "        \n",
    "       \n",
    "        self.imagen = model.imagen\n",
    "       \n",
    "        # elucidated or not\n",
    "\n",
    "        self.model=model\n",
    "        self.is_elucidated = self.model.is_elucidated\n",
    "        # create accelerator instance\n",
    "\n",
    "        accelerate_kwargs, kwargs = groupby_prefix_and_trim('accelerate_', kwargs)\n",
    "\n",
    "        assert not (fp16 and exists(precision)), 'either set fp16 = True or forward the precision (\"fp16\", \"bf16\") to Accelerator'\n",
    "        accelerator_mixed_precision = default(precision, 'fp16' if fp16 else 'no')\n",
    "\n",
    "        self.accelerator = Accelerator(**{\n",
    "            'split_batches': split_batches,\n",
    "            'mixed_precision': accelerator_mixed_precision,\n",
    "            'kwargs_handlers': [DistributedDataParallelKwargs(find_unused_parameters = True)]\n",
    "        , **accelerate_kwargs})\n",
    "\n",
    "        ImagenTrainer.locked = self.is_distributed\n",
    "\n",
    "        # cast data to fp16 at training time if needed\n",
    "\n",
    "        self.cast_half_at_training = accelerator_mixed_precision == 'fp16'\n",
    "\n",
    "        # grad scaler must be managed outside of accelerator\n",
    "\n",
    "        grad_scaler_enabled = fp16\n",
    "\n",
    "        # imagen, unets and ema unets\n",
    "\n",
    "         \n",
    "        self.num_unets = len(self.imagen.unets)\n",
    "\n",
    "        self.use_ema = use_ema and self.is_main\n",
    "        self.ema_unets = nn.ModuleList([])\n",
    "\n",
    "        # keep track of what unet is being trained on\n",
    "        # only going to allow 1 unet training at a time\n",
    "\n",
    "        self.ema_unet_being_trained_index = -1 # keeps track of which ema unet is being trained on\n",
    "\n",
    "        # data related functions\n",
    "\n",
    "        self.train_dl_iter = None\n",
    "        self.train_dl = None\n",
    "\n",
    "        self.valid_dl_iter = None\n",
    "        self.valid_dl = None\n",
    "\n",
    "        self.dl_tuple_output_keywords_names = dl_tuple_output_keywords_names\n",
    "\n",
    "        # auto splitting validation from training, if dataset is passed in\n",
    "\n",
    "        self.split_valid_from_train = split_valid_from_train\n",
    "\n",
    "        assert 0 <= split_valid_fraction <= 1, 'split valid fraction must be between 0 and 1'\n",
    "        self.split_valid_fraction = split_valid_fraction\n",
    "        self.split_random_seed = split_random_seed\n",
    "\n",
    "        # be able to finely customize learning rate, weight decay\n",
    "        # per unet\n",
    "\n",
    "        lr, eps, warmup_steps, cosine_decay_max_steps = map(partial(cast_tuple, length = self.num_unets), (lr, eps, warmup_steps, cosine_decay_max_steps))\n",
    "\n",
    "        for ind, (unet, unet_lr, unet_eps, unet_warmup_steps, unet_cosine_decay_max_steps) in enumerate(zip(self.imagen.unets, lr, eps, warmup_steps, cosine_decay_max_steps)):\n",
    "            optimizer = Adam(\n",
    "                unet.parameters(),\n",
    "                lr = unet_lr,\n",
    "                eps = unet_eps,\n",
    "                betas = (beta1, beta2),\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "            if self.use_ema:\n",
    "                self.ema_unets.append(EMA(unet, **ema_kwargs))\n",
    "\n",
    "            scaler = GradScaler(enabled = grad_scaler_enabled)\n",
    "\n",
    "            scheduler = warmup_scheduler = None\n",
    "\n",
    "            if exists(unet_cosine_decay_max_steps):\n",
    "                scheduler = CosineAnnealingLR(optimizer, T_max = unet_cosine_decay_max_steps)\n",
    "\n",
    "            if exists(unet_warmup_steps):\n",
    "                warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period = unet_warmup_steps)\n",
    "\n",
    "                if not exists(scheduler):\n",
    "                    scheduler = LambdaLR(optimizer, lr_lambda = lambda step: 1.0)\n",
    "\n",
    "            # set on object\n",
    "\n",
    "            setattr(self, f'optim{ind}', optimizer) # cannot use pytorch ModuleList for some reason with optimizers\n",
    "            setattr(self, f'scaler{ind}', scaler)\n",
    "            setattr(self, f'scheduler{ind}', scheduler)\n",
    "            setattr(self, f'warmup{ind}', warmup_scheduler)\n",
    "\n",
    "        # gradient clipping if needed\n",
    "\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        # step tracker and misc\n",
    "\n",
    "        self.register_buffer('steps', torch.tensor([0] * self.num_unets))\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # automatic set devices based on what accelerator decided\n",
    "\n",
    "        self.imagen.to(self.device)\n",
    "        self.to(self.device)\n",
    "\n",
    "        # checkpointing\n",
    "\n",
    "        assert not (exists(checkpoint_path) ^ exists(checkpoint_every))\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.checkpoint_every = checkpoint_every\n",
    "        self.max_checkpoints_keep = max_checkpoints_keep\n",
    "\n",
    "        self.can_checkpoint = self.is_local_main if isinstance(checkpoint_fs, LocalFileSystem) else self.is_main\n",
    "\n",
    "        if exists(checkpoint_path) and self.can_checkpoint:\n",
    "            bucket = url_to_bucket(checkpoint_path)\n",
    "\n",
    "            if not self.fs.exists(bucket):\n",
    "                self.fs.mkdir(bucket)\n",
    "\n",
    "            self.load_from_checkpoint_folder()\n",
    "\n",
    "        # only allowing training for unet\n",
    "\n",
    "        self.only_train_unet_number = only_train_unet_number\n",
    "        self.validate_and_set_unet_being_trained(only_train_unet_number)\n",
    "\n",
    "    # computed values\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.accelerator.device\n",
    "\n",
    "    @property\n",
    "    def is_distributed(self):\n",
    "        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n",
    "\n",
    "    @property\n",
    "    def is_main(self):\n",
    "        return self.accelerator.is_main_process\n",
    "\n",
    "    @property\n",
    "    def is_local_main(self):\n",
    "        return self.accelerator.is_local_main_process\n",
    "\n",
    "    @property\n",
    "    def unwrapped_unet(self):\n",
    "        return self.accelerator.unwrap_model(self.unet_being_trained)\n",
    "\n",
    "    # optimizer helper functions\n",
    "\n",
    "    def get_lr(self, unet_number):\n",
    "        self.validate_unet_number(unet_number)\n",
    "        unet_index = unet_number - 1\n",
    "\n",
    "        optim = getattr(self, f'optim{unet_index}')\n",
    "\n",
    "        return optim.param_groups[0]['lr']\n",
    "\n",
    "    # function for allowing only one unet from being trained at a time\n",
    "\n",
    "    def validate_and_set_unet_being_trained(self, unet_number = None):\n",
    "        if exists(unet_number):\n",
    "            self.validate_unet_number(unet_number)\n",
    "\n",
    "        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, 'you cannot only train on one unet at a time. you will need to save the trainer into a checkpoint, and resume training on a new unet'\n",
    "\n",
    "        self.only_train_unet_number = unet_number\n",
    "        self.imagen.only_train_unet_number = unet_number\n",
    "\n",
    "        if not exists(unet_number):\n",
    "            return\n",
    "\n",
    "        self.wrap_unet(unet_number)\n",
    "\n",
    "    def wrap_unet(self, unet_number):\n",
    "        if hasattr(self, 'one_unet_wrapped'):\n",
    "            return\n",
    "\n",
    "        unet = self.imagen.get_unet(unet_number)\n",
    "        self.unet_being_trained = self.accelerator.prepare(unet)\n",
    "        unet_index = unet_number - 1\n",
    "\n",
    "        optimizer = getattr(self, f'optim{unet_index}')\n",
    "        scheduler = getattr(self, f'scheduler{unet_index}')\n",
    "\n",
    "        optimizer = self.accelerator.prepare(optimizer)\n",
    "\n",
    "        if exists(scheduler):\n",
    "            scheduler = self.accelerator.prepare(scheduler)\n",
    "\n",
    "        setattr(self, f'optim{unet_index}', optimizer)\n",
    "        setattr(self, f'scheduler{unet_index}', scheduler)\n",
    "\n",
    "        self.one_unet_wrapped = True\n",
    "\n",
    "    # hacking accelerator due to not having separate gradscaler per optimizer\n",
    "\n",
    "    def set_accelerator_scaler(self, unet_number):\n",
    "        unet_number = self.validate_unet_number(unet_number)\n",
    "        scaler = getattr(self, f'scaler{unet_number - 1}')\n",
    "\n",
    "        self.accelerator.scaler = scaler\n",
    "        for optimizer in self.accelerator._optimizers:\n",
    "            optimizer.scaler = scaler\n",
    "\n",
    "    # helper print\n",
    "\n",
    "    def print(self, msg):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        if not self.verbose:\n",
    "            return\n",
    "\n",
    "        return self.accelerator.print(msg)\n",
    "\n",
    "    # validating the unet number\n",
    "\n",
    "    def validate_unet_number(self, unet_number = None):\n",
    "        if self.num_unets == 1:\n",
    "            unet_number = default(unet_number, 1)\n",
    "\n",
    "        assert 0 < unet_number <= self.num_unets, f'unet number should be in between 1 and {self.num_unets}'\n",
    "        return unet_number\n",
    "\n",
    "    # number of training steps taken\n",
    "\n",
    "    def num_steps_taken(self, unet_number = None):\n",
    "        if self.num_unets == 1:\n",
    "            unet_number = default(unet_number, 1)\n",
    "\n",
    "        return self.steps[unet_number - 1].item()\n",
    "\n",
    "    def print_untrained_unets(self):\n",
    "        print_final_error = False\n",
    "\n",
    "        for ind, (steps, unet) in enumerate(zip(self.steps.tolist(), self.imagen.unets)):\n",
    "            if steps > 0 or isinstance(unet, NullUnet):\n",
    "                continue\n",
    "\n",
    "            self.print(f'unet {ind + 1} has not been trained')\n",
    "            print_final_error = True\n",
    "\n",
    "        if print_final_error:\n",
    "            self.print('when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets')\n",
    "\n",
    "    # data related functions\n",
    "\n",
    "    def add_train_dataloader(self, dl = None):\n",
    "        if not exists(dl):\n",
    "            return\n",
    "\n",
    "        assert not exists(self.train_dl), 'training dataloader was already added'\n",
    "        self.train_dl = self.accelerator.prepare(dl)\n",
    "\n",
    "    def add_valid_dataloader(self, dl):\n",
    "        if not exists(dl):\n",
    "            return\n",
    "\n",
    "        assert not exists(self.valid_dl), 'validation dataloader was already added'\n",
    "        self.valid_dl = self.accelerator.prepare(dl)\n",
    "\n",
    "    def add_train_dataset(self, ds = None, *, batch_size, **dl_kwargs):\n",
    "        if not exists(ds):\n",
    "            return\n",
    "\n",
    "        assert not exists(self.train_dl), 'training dataloader was already added'\n",
    "\n",
    "        valid_ds = None\n",
    "        if self.split_valid_from_train:\n",
    "            train_size = int((1 - self.split_valid_fraction) * len(ds))\n",
    "            valid_size = len(ds) - train_size\n",
    "\n",
    "            ds, valid_ds = random_split(ds, [train_size, valid_size], generator = torch.Generator().manual_seed(self.split_random_seed))\n",
    "            self.print(f'training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples')\n",
    "\n",
    "        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n",
    "        self.train_dl = self.accelerator.prepare(dl)\n",
    "\n",
    "        if not self.split_valid_from_train:\n",
    "            return\n",
    "\n",
    "        self.add_valid_dataset(valid_ds, batch_size = batch_size, **dl_kwargs)\n",
    "\n",
    "    def add_valid_dataset(self, ds, *, batch_size, **dl_kwargs):\n",
    "        if not exists(ds):\n",
    "            return\n",
    "\n",
    "        assert not exists(self.valid_dl), 'validation dataloader was already added'\n",
    "\n",
    "        dl = DataLoader(ds, batch_size = batch_size, **dl_kwargs)\n",
    "        self.valid_dl = self.accelerator.prepare(dl)\n",
    "\n",
    "    def create_train_iter(self):\n",
    "        assert exists(self.train_dl), 'training dataloader has not been registered with the trainer yet'\n",
    "\n",
    "        if exists(self.train_dl_iter):\n",
    "            return\n",
    "\n",
    "        self.train_dl_iter = cycle(self.train_dl)\n",
    "\n",
    "    def create_valid_iter(self):\n",
    "        assert exists(self.valid_dl), 'validation dataloader has not been registered with the trainer yet'\n",
    "\n",
    "        if exists(self.valid_dl_iter):\n",
    "            return\n",
    "\n",
    "        self.valid_dl_iter = cycle(self.valid_dl)\n",
    "\n",
    "    def train_step(self, unet_number = None, **kwargs):\n",
    "        self.create_train_iter()\n",
    "        loss = self.step_with_dl_iter(self.train_dl_iter, unet_number = unet_number, **kwargs)\n",
    "        self.update(unet_number = unet_number)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @eval_decorator\n",
    "    def valid_step(self, **kwargs):\n",
    "        self.create_valid_iter()\n",
    "\n",
    "        context = self.use_ema_unets if kwargs.pop('use_ema_unets', False) else nullcontext\n",
    "\n",
    "        with context():\n",
    "            loss = self.step_with_dl_iter(self.valid_dl_iter, **kwargs)\n",
    "        return loss\n",
    "\n",
    "    def step_with_dl_iter(self, dl_iter, **kwargs):\n",
    "        dl_tuple_output = cast_tuple(next(dl_iter))\n",
    "        model_input = dict(list(zip(self.dl_tuple_output_keywords_names, dl_tuple_output)))\n",
    "        loss = self.forward(**{**kwargs, **model_input})\n",
    "        return loss\n",
    "\n",
    "    # checkpointing functions\n",
    "\n",
    "    @property\n",
    "    def all_checkpoints_sorted(self):\n",
    "        glob_pattern = os.path.join(self.checkpoint_path, '*.pt')\n",
    "        checkpoints = self.fs.glob(glob_pattern)\n",
    "        sorted_checkpoints = sorted(checkpoints, key = lambda x: int(str(x).split('.')[-2]), reverse = True)\n",
    "        return sorted_checkpoints\n",
    "\n",
    "    def load_from_checkpoint_folder(self, last_total_steps = -1):\n",
    "        if last_total_steps != -1:\n",
    "            filepath = os.path.join(self.checkpoint_path, f'checkpoint.{last_total_steps}.pt')\n",
    "            self.load(filepath)\n",
    "            return\n",
    "\n",
    "        sorted_checkpoints = self.all_checkpoints_sorted\n",
    "\n",
    "        if len(sorted_checkpoints) == 0:\n",
    "            self.print(f'no checkpoints found to load from at {self.checkpoint_path}')\n",
    "            return\n",
    "\n",
    "        last_checkpoint = sorted_checkpoints[0]\n",
    "        self.load(last_checkpoint)\n",
    "\n",
    "    def save_to_checkpoint_folder(self):\n",
    "        self.accelerator.wait_for_everyone()\n",
    "\n",
    "        if not self.can_checkpoint:\n",
    "            return\n",
    "\n",
    "        total_steps = int(self.steps.sum().item())\n",
    "        filepath = os.path.join(self.checkpoint_path, f'checkpoint.{total_steps}.pt')\n",
    "\n",
    "        self.save(filepath)\n",
    "\n",
    "        if self.max_checkpoints_keep <= 0:\n",
    "            return\n",
    "\n",
    "        sorted_checkpoints = self.all_checkpoints_sorted\n",
    "        checkpoints_to_discard = sorted_checkpoints[self.max_checkpoints_keep:]\n",
    "\n",
    "        for checkpoint in checkpoints_to_discard:\n",
    "            self.fs.rm(checkpoint)\n",
    "\n",
    "    # saving and loading functions\n",
    "\n",
    "    def save(\n",
    "        self,\n",
    "        path,\n",
    "        overwrite = True,\n",
    "        without_optim_and_sched = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.accelerator.wait_for_everyone()\n",
    "\n",
    "        if not self.can_checkpoint:\n",
    "            return\n",
    "\n",
    "        fs = self.fs\n",
    "\n",
    "        assert not (fs.exists(path) and not overwrite)\n",
    "\n",
    "        self.reset_ema_unets_all_one_device()\n",
    "\n",
    "        save_obj = dict(\n",
    "            model = self.imagen.state_dict(),\n",
    "            version = __version__,\n",
    "            steps = self.steps.cpu(),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        save_optim_and_sched_iter = range(0, self.num_unets) if not without_optim_and_sched else tuple()\n",
    "\n",
    "        for ind in save_optim_and_sched_iter:\n",
    "            scaler_key = f'scaler{ind}'\n",
    "            optimizer_key = f'optim{ind}'\n",
    "            scheduler_key = f'scheduler{ind}'\n",
    "            warmup_scheduler_key = f'warmup{ind}'\n",
    "\n",
    "            scaler = getattr(self, scaler_key)\n",
    "            optimizer = getattr(self, optimizer_key)\n",
    "            scheduler = getattr(self, scheduler_key)\n",
    "            warmup_scheduler = getattr(self, warmup_scheduler_key)\n",
    "\n",
    "            if exists(scheduler):\n",
    "                save_obj = {**save_obj, scheduler_key: scheduler.state_dict()}\n",
    "\n",
    "            if exists(warmup_scheduler):\n",
    "                save_obj = {**save_obj, warmup_scheduler_key: warmup_scheduler.state_dict()}\n",
    "\n",
    "            save_obj = {**save_obj, scaler_key: scaler.state_dict(), optimizer_key: optimizer.state_dict()}\n",
    "\n",
    "        if self.use_ema:\n",
    "            save_obj = {**save_obj, 'ema': self.ema_unets.state_dict()}\n",
    "\n",
    "        # determine if imagen config is available\n",
    "\n",
    "        if hasattr(self.imagen, '_config'):\n",
    "            self.print(f'this checkpoint is commandable from the CLI - \"imagen --model {str(path)} \\\"<prompt>\\\"\"')\n",
    "\n",
    "            save_obj = {\n",
    "                **save_obj,\n",
    "                'imagen_type': 'elucidated' if self.is_elucidated else 'original',\n",
    "                'imagen_params': self.imagen._config\n",
    "            }\n",
    "\n",
    "        #save to path\n",
    "\n",
    "        with fs.open(path, 'wb') as f:\n",
    "            torch.save(save_obj, f)\n",
    "\n",
    "        self.print(f'checkpoint saved to {path}')\n",
    "\n",
    "    def load(self, path, only_model = False, strict = True, noop_if_not_exist = False):\n",
    "        fs = self.fs\n",
    "\n",
    "        if noop_if_not_exist and not fs.exists(path):\n",
    "            self.print(f'trainer checkpoint not found at {str(path)}')\n",
    "            return\n",
    "\n",
    "        assert fs.exists(path), f'{path} does not exist'\n",
    "\n",
    "        self.reset_ema_unets_all_one_device()\n",
    "\n",
    "        # to avoid extra GPU memory usage in main process when using Accelerate\n",
    "\n",
    "        with fs.open(path) as f:\n",
    "            loaded_obj = torch.load(f, map_location='cpu')\n",
    "\n",
    "        if version.parse(__version__) != version.parse(loaded_obj['version']):\n",
    "            self.print(f'loading saved imagen at version {loaded_obj[\"version\"]}, but current package version is {__version__}')\n",
    "\n",
    "        try:\n",
    "            self.imagen.load_state_dict(loaded_obj['model'], strict = strict)\n",
    "        except RuntimeError:\n",
    "            print(\"Failed loading state dict. Trying partial load\")\n",
    "            self.imagen.load_state_dict(restore_parts(self.imagen.state_dict(),\n",
    "                                                      loaded_obj['model']))\n",
    "\n",
    "        if only_model:\n",
    "            return loaded_obj\n",
    "\n",
    "        self.steps.copy_(loaded_obj['steps'])\n",
    "\n",
    "        for ind in range(0, self.num_unets):\n",
    "            scaler_key = f'scaler{ind}'\n",
    "            optimizer_key = f'optim{ind}'\n",
    "            scheduler_key = f'scheduler{ind}'\n",
    "            warmup_scheduler_key = f'warmup{ind}'\n",
    "\n",
    "            scaler = getattr(self, scaler_key)\n",
    "            optimizer = getattr(self, optimizer_key)\n",
    "            scheduler = getattr(self, scheduler_key)\n",
    "            warmup_scheduler = getattr(self, warmup_scheduler_key)\n",
    "\n",
    "            if exists(scheduler) and scheduler_key in loaded_obj:\n",
    "                scheduler.load_state_dict(loaded_obj[scheduler_key])\n",
    "\n",
    "            if exists(warmup_scheduler) and warmup_scheduler_key in loaded_obj:\n",
    "                warmup_scheduler.load_state_dict(loaded_obj[warmup_scheduler_key])\n",
    "\n",
    "            if exists(optimizer):\n",
    "                try:\n",
    "                    optimizer.load_state_dict(loaded_obj[optimizer_key])\n",
    "                    scaler.load_state_dict(loaded_obj[scaler_key])\n",
    "                except:\n",
    "                    self.print('could not load optimizer and scaler, possibly because you have turned on mixed precision training since the last run. resuming with new optimizer and scalers')\n",
    "\n",
    "        if self.use_ema:\n",
    "            assert 'ema' in loaded_obj\n",
    "            try:\n",
    "                self.ema_unets.load_state_dict(loaded_obj['ema'], strict = strict)\n",
    "            except RuntimeError:\n",
    "                print(\"Failed loading state dict. Trying partial load\")\n",
    "                self.ema_unets.load_state_dict(restore_parts(self.ema_unets.state_dict(),\n",
    "                                                             loaded_obj['ema']))\n",
    "\n",
    "        self.print(f'checkpoint loaded from {path}')\n",
    "        return loaded_obj\n",
    "\n",
    "    # managing ema unets and their devices\n",
    "\n",
    "    @property\n",
    "    def unets(self):\n",
    "        return nn.ModuleList([ema.ema_model for ema in self.ema_unets])\n",
    "\n",
    "    def get_ema_unet(self, unet_number = None):\n",
    "        if not self.use_ema:\n",
    "            return\n",
    "\n",
    "        unet_number = self.validate_unet_number(unet_number)\n",
    "        index = unet_number - 1\n",
    "\n",
    "        if isinstance(self.unets, nn.ModuleList):\n",
    "            unets_list = [unet for unet in self.ema_unets]\n",
    "            delattr(self, 'ema_unets')\n",
    "            self.ema_unets = unets_list\n",
    "\n",
    "        if index != self.ema_unet_being_trained_index:\n",
    "            for unet_index, unet in enumerate(self.ema_unets):\n",
    "                unet.to(self.device if unet_index == index else 'cpu')\n",
    "\n",
    "        self.ema_unet_being_trained_index = index\n",
    "        return self.ema_unets[index]\n",
    "\n",
    "    def reset_ema_unets_all_one_device(self, device = None):\n",
    "        if not self.use_ema:\n",
    "            return\n",
    "\n",
    "        device = default(device, self.device)\n",
    "        self.ema_unets = nn.ModuleList([*self.ema_unets])\n",
    "        self.ema_unets.to(device)\n",
    "\n",
    "        self.ema_unet_being_trained_index = -1\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @contextmanager\n",
    "    def use_ema_unets(self):\n",
    "        if not self.use_ema:\n",
    "            output = yield\n",
    "            return output\n",
    "\n",
    "        self.reset_ema_unets_all_one_device()\n",
    "        self.imagen.reset_unets_all_one_device()\n",
    "\n",
    "        self.unets.eval()\n",
    "\n",
    "        trainable_unets = self.imagen.unets\n",
    "        self.imagen.unets = self.unets                  # swap in exponential moving averaged unets for sampling\n",
    "\n",
    "        output = yield\n",
    "\n",
    "        self.imagen.unets = trainable_unets             # restore original training unets\n",
    "\n",
    "        # cast the ema_model unets back to original device\n",
    "        for ema in self.ema_unets:\n",
    "            ema.restore_ema_model_device()\n",
    "\n",
    "        return output\n",
    "\n",
    "    def print_unet_devices(self):\n",
    "        self.print('unet devices:')\n",
    "        for i, unet in enumerate(self.imagen.unets):\n",
    "            device = next(unet.parameters()).device\n",
    "            self.print(f'\\tunet {i}: {device}')\n",
    "\n",
    "        if not self.use_ema:\n",
    "            return\n",
    "\n",
    "        self.print('\\nema unet devices:')\n",
    "        for i, ema_unet in enumerate(self.ema_unets):\n",
    "            device = next(ema_unet.parameters()).device\n",
    "            self.print(f'\\tema unet {i}: {device}')\n",
    "\n",
    "    # overriding state dict functions\n",
    "\n",
    "    def state_dict(self, *args, **kwargs):\n",
    "        self.reset_ema_unets_all_one_device()\n",
    "        return super().state_dict(*args, **kwargs)\n",
    "\n",
    "    def load_state_dict(self, *args, **kwargs):\n",
    "        self.reset_ema_unets_all_one_device()\n",
    "        return super().load_state_dict(*args, **kwargs)\n",
    "\n",
    "    # encoding text functions\n",
    "\n",
    "    def encode_text(self, text, **kwargs):\n",
    "        return self.imagen.encode_text(text, **kwargs)\n",
    "\n",
    "    # forwarding functions and gradient step updates\n",
    "\n",
    "    def update(self, unet_number = None):\n",
    "        unet_number = self.validate_unet_number(unet_number)\n",
    "        self.validate_and_set_unet_being_trained(unet_number)\n",
    "        self.set_accelerator_scaler(unet_number)\n",
    "\n",
    "        index = unet_number - 1\n",
    "        unet = self.unet_being_trained\n",
    "\n",
    "        optimizer = getattr(self, f'optim{index}')\n",
    "        scaler = getattr(self, f'scaler{index}')\n",
    "        scheduler = getattr(self, f'scheduler{index}')\n",
    "        warmup_scheduler = getattr(self, f'warmup{index}')\n",
    "\n",
    "        # set the grad scaler on the accelerator, since we are managing one per u-net\n",
    "\n",
    "        if exists(self.max_grad_norm):\n",
    "            self.accelerator.clip_grad_norm_(unet.parameters(), self.max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if self.use_ema:\n",
    "            ema_unet = self.get_ema_unet(unet_number)\n",
    "            ema_unet.update()\n",
    "\n",
    "        # scheduler, if needed\n",
    "\n",
    "        maybe_warmup_context = nullcontext() if not exists(warmup_scheduler) else warmup_scheduler.dampening()\n",
    "\n",
    "        with maybe_warmup_context:\n",
    "            if exists(scheduler) and not self.accelerator.optimizer_step_was_skipped: # recommended in the docs\n",
    "                scheduler.step()\n",
    "\n",
    "        self.steps += F.one_hot(torch.tensor(unet_number - 1, device = self.steps.device), num_classes = len(self.steps))\n",
    "\n",
    "        if not exists(self.checkpoint_path):\n",
    "            return\n",
    "\n",
    "        total_steps = int(self.steps.sum().item())\n",
    "\n",
    "        if total_steps % self.checkpoint_every:\n",
    "            return\n",
    "\n",
    "        self.save_to_checkpoint_folder()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @cast_torch_tensor\n",
    "    @imagen_sample_in_chunks\n",
    "    def sample(self, *args, **kwargs):\n",
    "        context = nullcontext if  kwargs.pop('use_non_ema', False) else self.use_ema_unets\n",
    "\n",
    "        self.print_untrained_unets()        \n",
    "        \n",
    "        if not self.is_main:\n",
    "            kwargs['use_tqdm'] = False\n",
    "\n",
    "        with context():\n",
    "            output = self.imagen.sample(*args, device = self.device, **kwargs)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @partial(cast_torch_tensor, cast_fp16 = True)\n",
    "    def forward(\n",
    "        self,\n",
    "        *args,\n",
    "        unet_number = None,\n",
    "        max_batch_size = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        unet_number = self.validate_unet_number(unet_number)\n",
    "        self.validate_and_set_unet_being_trained(unet_number)\n",
    "        self.set_accelerator_scaler(unet_number)\n",
    "\n",
    "        assert not exists(self.only_train_unet_number) or self.only_train_unet_number == unet_number, f'you can only train unet #{self.only_train_unet_number}'\n",
    "\n",
    "        total_loss = 0.\n",
    "\n",
    "        for chunk_size_frac, (chunked_args, chunked_kwargs) in split_args_and_kwargs(*args, split_size = max_batch_size, **kwargs):\n",
    "            with self.accelerator.autocast():\n",
    "                loss = self.model(*chunked_args, unet_number = unet_number, **chunked_kwargs)\n",
    "                loss = loss * chunk_size_frac\n",
    "\n",
    "            total_loss += loss#.item()\n",
    "\n",
    "            if self.training:\n",
    "                self.accelerator.backward(loss)\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d372dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fasta (sequence, filename_out):\n",
    "    \n",
    "    with open (filename_out, mode ='w') as f:\n",
    "        f.write (f'>{filename_out}\\n')\n",
    "        f.write (f'{sequence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81511cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence (model,\n",
    "                X=[[0.92, 0., 0.04, 0.04, 0., 0., 0., 0., ]],\n",
    "                 flag=0,\n",
    "                     cond_scales=1.,foldproteins=False,\n",
    "                     normalize_input_to_one=False,\n",
    "                     calc_error=False,\n",
    "               ):\n",
    "    steps=0\n",
    "    e=flag\n",
    "\n",
    "\n",
    "    print (f\"Producing {len(X)} samples...\")\n",
    "    \n",
    "    print ('Device: ', device)\n",
    "\n",
    "\n",
    "    for iisample in range (len (X)):\n",
    "        \n",
    "        X_cond=torch.Tensor (X[iisample]).to(device).unsqueeze (0)\n",
    "        \n",
    "        if normalize_input_to_one:\n",
    "            X_cond=X_cond/X_cond.sum()\n",
    "        \n",
    "        print (\"Conditoning used: \", X_cond, \"...sum: \", X_cond.sum(), \"cond scale: \", cond_scales)\n",
    "        \n",
    "        result=model.sample ( X_cond,stop_at_unet_number=train_unet_number ,\n",
    "                                 cond_scale=cond_scales )\n",
    "            \n",
    "        result=torch.round(result*ynormfac)\n",
    "        plt.plot (result[0,0,:].cpu().detach().numpy(),label= f'Predicted')\n",
    "        \n",
    "        plt.legend()\n",
    "\n",
    "        outname = prefix+ f\"sampld_from_X_{flag}_condscale-{str (cond_scales)}_{e}_{steps}.jpg\"\n",
    "       \n",
    "        plt.savefig(outname, dpi=200)\n",
    "        plt.show ()\n",
    "\n",
    "        to_rev=result[:,0,:]\n",
    "        to_rev=to_rev.long().cpu().detach().numpy()\n",
    "        \n",
    "        y_data_reversed=tokenizer_y.sequences_to_texts (to_rev)\n",
    "\n",
    "        for iii in range (len(y_data_reversed)):\n",
    "            y_data_reversed[iii]=y_data_reversed[iii].upper().strip().replace(\" \", \"\")\n",
    "\n",
    "        print (f\"For {X}, predicted sequence\", y_data_reversed[0])\n",
    "        \n",
    "         \n",
    "        if foldproteins:\n",
    "            \n",
    "            xbc=X_cond[iisample,:].cpu().detach().numpy()\n",
    "            out_nam=np.array2string(xbc, formatter={'float_kind':lambda xbc: \"%.2f\" % xbc})+f'_{flag}_{steps}'\n",
    "            tempname='temp'\n",
    "            pdb_file=foldandsavePDB (sequence=y_data_reversed[0], \n",
    "                                                 filename_out=tempname, \n",
    "                                                 num_cycle=16, flag=flag)\n",
    "            out_nam_fasta=f'{prefix}{out_nam}.fasta'\n",
    "            \n",
    "            out_nam=f'{prefix}{out_nam}.pdb'\n",
    "            \n",
    "            write_fasta (y_data_reversed[0], out_nam_fasta)\n",
    "            \n",
    "            shutil.copy (pdb_file, out_nam) #source, dest\n",
    "            pdb_file=out_nam\n",
    "            print (f\"Properly named PDB file produced: {pdb_file}\")\n",
    "          \n",
    "            view=show_pdb(pdb_file=pdb_file, flag=flag,\n",
    "                          show_sidechains=show_sidechains, show_mainchains=show_mainchains, color=color)\n",
    "            view.show()\n",
    "\n",
    "        if calc_error:\n",
    "            get_Model_A_error (pdb_file, X[iisample], plotit=True)\n",
    "        return pdb_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_loop (model,\n",
    "                train_loader,\n",
    "                cond_scales=[1.0], #list of cond scales - each sampled...\n",
    "                num_samples=2, #how many samples produced every time tested.....\n",
    "                timesteps=100,\n",
    "                 flag=0,foldproteins=False,\n",
    "                 calc_error=False,\n",
    "               ):\n",
    "    steps=0\n",
    "    e=flag\n",
    "    for item  in train_loader:\n",
    "\n",
    "\n",
    "            X_train_batch= item[0].to(device)\n",
    "            y_train_batch=item[1].to(device)\n",
    "\n",
    "\n",
    "            GT=y_train_batch.cpu().detach() \n",
    "            \n",
    "            GT=resize_image_to(\n",
    "                GT.unsqueeze(1),\n",
    "              \n",
    "                model.imagen.image_sizes[train_unet_number-1],\n",
    "\n",
    "            )\n",
    "           \n",
    "            num_samples = min (num_samples,y_train_batch.shape[0] )\n",
    "            print (f\"Producing {num_samples} samples...\")\n",
    " \n",
    "\n",
    "            for iisample in range (len (cond_scales)):\n",
    "                \n",
    "                result=model.sample ( X_train_batch,stop_at_unet_number=train_unet_number ,\n",
    "                                         cond_scale=cond_scales[iisample])\n",
    "                \n",
    "                \n",
    "                result=torch.round(result*ynormfac)    \n",
    "                \n",
    "                GT=torch.round(GT*ynormfac)    \n",
    "                for samples in range  (num_samples):\n",
    "                    print (\"sample \", samples, \"out of \", num_samples)\n",
    "                    \n",
    "                    plt.plot (result[samples,0,:].cpu().detach().numpy(),label= f'Predicted')\n",
    "                    plt.plot (GT[samples,0,:],label= f'GT {0}')\n",
    "                    plt.legend()\n",
    "\n",
    "                    outname = prefix+ f\"sample-{samples}_condscale-{str (cond_scales[iisample])}_{e}_{steps}.jpg\"\n",
    "                  \n",
    "                    plt.savefig(outname, dpi=200)\n",
    "                    plt.show ()\n",
    "                    \n",
    "                    to_rev=result[:,0,:]\n",
    "                    to_rev=to_rev.long().cpu().detach().numpy()\n",
    "                   \n",
    "                    y_data_reversed=tokenizer_y.sequences_to_texts (to_rev)\n",
    "\n",
    "                    for iii in range (len(y_data_reversed)):\n",
    "                        y_data_reversed[iii]=y_data_reversed[iii].upper().strip().replace(\" \", \"\")\n",
    "\n",
    "                    print (f\"For {X_train_batch[samples,:].cpu().detach().numpy()}, predicted sequence\", y_data_reversed[samples])\n",
    "                    if foldproteins:\n",
    "                        xbc=X_train_batch[samples,:].cpu().detach().numpy()\n",
    "                        out_nam=np.array2string(xbc, formatter={'float_kind':lambda xbc: \"%.1f\" % xbc})+f'_{flag}_{samples}'\n",
    "                        tempname='temp'\n",
    "                        pdb_file=foldandsavePDB (sequence=y_data_reversed[samples], \n",
    "                                                             filename_out=tempname, \n",
    "                                                             num_cycle=16, flag=flag)\n",
    "                        out_nam=f'{prefix}{out_nam}.pdb'\n",
    "                        print (f'Original PDB: {pdb_file} OUT: {out_nam}')\n",
    "                        shutil.copy (pdb_file, out_nam) #source, dest\n",
    "                        pdb_file=out_nam\n",
    "                        print (f\"Properly named PDB file produced: {pdb_file}\")\n",
    "                        \n",
    "                        view=show_pdb(pdb_file=pdb_file, flag=flag,\n",
    "                                      show_sidechains=show_sidechains, show_mainchains=show_mainchains, color=color)\n",
    "                        view.show()\n",
    "\n",
    "                    steps=steps+1\n",
    "                    \n",
    "                    if calc_error:\n",
    "                        cond=X_train_batch[samples,:].cpu().detach().numpy()  #X_train_batch[samples,:] \n",
    "                        get_Model_A_error (pdb_file, cond, plotit=True)\n",
    "\n",
    "            if steps>num_samples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b2cc38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_loop (model,\n",
    "                train_loader,\n",
    "                test_loader,\n",
    "                optimizer=None,\n",
    "                print_every=10,\n",
    "                epochs= 300,\n",
    "                start_ep=0,\n",
    "                start_step=0,\n",
    "                train_unet_number=1,\n",
    "                print_loss=1000,\n",
    "                trainer=None,\n",
    "                plot_unscaled=False,\n",
    "                max_batch_size=4,\n",
    "                save_model=False,\n",
    "                cond_scales=[1.0], #list of cond scales\n",
    "                num_samples=2, #how many samples produced every time tested.....\n",
    "                foldproteins=False,\n",
    "               ):\n",
    "    \n",
    "    \n",
    "    if not exists (trainer):\n",
    "        if not exists (optimizer):\n",
    "            print (\"ERROR: If trainer not used, need to provide optimizer.\")\n",
    "    if exists (trainer):\n",
    "        print (\"Trainer provided... will be used\")\n",
    "    steps=start_step\n",
    "\n",
    "    loss_total=0\n",
    "    for e in range(1, epochs+1):\n",
    "            start = time.time()\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            print (\"######################################################################################\")\n",
    "            start = time.time()\n",
    "            print (\"NOW: Training epoch: \", e+start_ep)\n",
    "\n",
    "            # TRAINING\n",
    "            train_epoch_loss = 0\n",
    "            model.train()\n",
    "            \n",
    "            print (\"Loop over \", len(train_loader), \" batches (print . every \", print_every, \" steps)\")\n",
    "            for item  in train_loader:\n",
    "                X_train_batch= item[0].to(device)\n",
    "                y_train_batch=item[1].to(device)\n",
    "\n",
    "                if exists (trainer):\n",
    "                    loss = trainer(\n",
    "                            X_train_batch, y_train_batch.unsqueeze(1) ,\n",
    "                            unet_number=train_unet_number,\n",
    "                            max_batch_size = max_batch_size,    # auto divide the batch of 64 up into batch size of 4 and accumulate gradients, so it all fits in memory\n",
    "                        )\n",
    "                    trainer.update(unet_number = train_unet_number)\n",
    "\n",
    "                else:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss=model ( X_train_batch, y_train_batch.unsqueeze(1) ,unet_number=train_unet_number)\n",
    "                    loss.backward( )\n",
    "                   \n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "                    optimizer.step()\n",
    "\n",
    "                loss_total=loss_total+loss.item()\n",
    "                \n",
    "                if steps % print_every == 0:\n",
    "                    print(\".\", end=\"\")\n",
    "\n",
    "                if steps>0:\n",
    "                    if steps % print_loss == 0:\n",
    "\n",
    "                        if plot_unscaled:\n",
    "                            \n",
    "                            plt.plot (y_train_batch.unsqueeze(1)[0,0,:].cpu().detach().numpy(),label= 'Unscaled GT')\n",
    "                            plt.legend()\n",
    "                            plt.show()\n",
    " \n",
    "                        #rescale GT to properly plot\n",
    "                        GT=y_train_batch.cpu().detach() \n",
    "                        \n",
    "                        GT=resize_image_to(\n",
    "                            GT.unsqueeze(1),\n",
    "                            model.imagen.image_sizes[train_unet_number-1],\n",
    "\n",
    "                        )\n",
    "                        \n",
    "                        norm_loss=loss_total/print_loss\n",
    "                        print (f\"\\nTOTAL LOSS at epoch={e}, step={steps}: {norm_loss}\")\n",
    "\n",
    "                        loss_list.append (norm_loss)\n",
    "                        loss_total=0\n",
    "\n",
    "                        plt.plot (loss_list, label='Loss')\n",
    "                        plt.legend()\n",
    "\n",
    "                        outname = prefix+ f\"loss_{e}_{steps}.jpg\"\n",
    "                        plt.savefig(outname, dpi=200)\n",
    "                        plt.show()\n",
    "                        \n",
    "                        num_samples = min (num_samples,y_train_batch.shape[0] )\n",
    "                        print (f\"Producing {num_samples} samples...\")\n",
    "                        \n",
    "                        sample_loop (model,\n",
    "                            test_loader,\n",
    "                            cond_scales=cond_scales,\n",
    "                            num_samples=1, #how many samples produced every time tested.....\n",
    "                            timesteps=64,\n",
    "                                    flag=steps,foldproteins=foldproteins,\n",
    "                                    )   \n",
    "                        \n",
    "                        print (\"SAMPLING FOR DE NOVO:\")\n",
    "                        sample_sequence (model,\n",
    "                            X=[[0, 0.7, 0.07, 0.1, 0.01, 0.02, 0.01, 0.11]],foldproteins=foldproteins,\n",
    "                             flag=steps,cond_scales=1.,\n",
    "                           )\n",
    "                        sample_sequence (model,\n",
    "                            X=[[0., 0.0, 0.0, 0.0, 0., 0., 0., 0., ]],foldproteins=foldproteins,\n",
    "                             flag=steps,cond_scales=1.,\n",
    "                           )\n",
    "\n",
    "                if steps>0:\n",
    "                    if save_model and steps % print_loss==0: \n",
    "                        fname=f\"{prefix}trainer_save-model-epoch_{e}.pt\"\n",
    "                        trainer.save(fname)\n",
    "                        fname=f\"{prefix}statedict_save-model-epoch_{e}.pt\"\n",
    "                        torch.save(model.state_dict(), fname)\n",
    "                        print (f\"Model saved: \")\n",
    "                    \n",
    "                steps=steps+1\n",
    "                                         \n",
    "            print (f\"\\n\\n-------------------\\nTime for epoch {e}={(time.time()-start)/60}\\n-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7704c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install py3Dmol\n",
    "#poymol vis: https://gist.github.com/bougui505/11401240    \n",
    "\n",
    "def foldandsavePDB (sequence, filename_out, num_cycle=16, flag=0):\n",
    "    filename=f\"{prefix}fasta_in_{flag}.fasta\"\n",
    "    print (\"Writing FASTA file: \", filename)\n",
    "    OUTFILE=f\"{filename_out}_{flag}\"\n",
    "    with open (filename, mode ='w') as f:\n",
    "        f.write (f'>{OUTFILE}\\n')\n",
    "        f.write (f'{sequence}')\n",
    "        \n",
    "    print (\"Now run OmegaFold....\")    \n",
    "    !omegafold $filename $prefix --num_cycle $num_cycle --device=$device\n",
    "    print (\"Done OmegaFold\")\n",
    "    \n",
    "    PDB_result=f\"{prefix}{OUTFILE}.pdb\"\n",
    "    print (f\"Resulting PDB file...:  {PDB_result}\")\n",
    "    \n",
    "    return PDB_result\n",
    "    \n",
    "import py3Dmol\n",
    "def plot_plddt_legend(dpi=100):\n",
    "  thresh = ['plDDT:','Very low (<50)','Low (60)','OK (70)','Confident (80)','Very high (>90)']\n",
    "  plt.figure(figsize=(1,0.1),dpi=dpi)\n",
    "  \n",
    "  for c in [\"#FFFFFF\",\"#FF0000\",\"#FFFF00\",\"#00FF00\",\"#00FFFF\",\"#0000FF\"]:\n",
    "    plt.bar(0, 0, color=c)\n",
    "  plt.legend(thresh, frameon=False,\n",
    "             loc='center', ncol=6,\n",
    "             handletextpad=1,\n",
    "             columnspacing=1,\n",
    "             markerscale=0.5,)\n",
    "  plt.axis(False)\n",
    "  return plt\n",
    "\n",
    "color = \"lDDT\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\n",
    "show_sidechains = False #@param {type:\"boolean\"}\n",
    "show_mainchains = False #@param {type:\"boolean\"}\n",
    "\n",
    "def show_pdb(pdb_file, flag=0,   show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n",
    "  model_name = f\"Flag_{flag}\"\n",
    "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
    "  view.addModel(open(pdb_file,'r').read(),'pdb')\n",
    "\n",
    "  if color == \"lDDT\":\n",
    "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
    "  elif color == \"rainbow\":\n",
    "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
    "  elif color == \"chain\":\n",
    "    chains = len(queries[0][1]) + 1 if is_complex else 1\n",
    "    for n,chain,color in zip(range(chains),list(\"ABCDEFGH\"),\n",
    "                     [\"lime\",\"cyan\",\"magenta\",\"yellow\",\"salmon\",\"white\",\"blue\",\"orange\"]):\n",
    "      view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
    "  if show_sidechains:\n",
    "    BB = ['C','O','N']\n",
    "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
    "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
    "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
    "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
    "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
    "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})  \n",
    "  if show_mainchains:\n",
    "    BB = ['C','O','N','CA']\n",
    "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
    "\n",
    "  view.zoomTo()\n",
    "  if color == \"lDDT\":\n",
    "      plot_plddt_legend().show() \n",
    "  return view\n",
    "\n",
    "\n",
    "def get_avg_Bfac (file='./output_v3/[0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0].pdb'):\n",
    "    p = PDBParser()\n",
    "    avg_B=0\n",
    "    bfac_list=[]\n",
    "    \n",
    "    structure = p.get_structure(\"X\", file)\n",
    "    for PDBmodel in structure:\n",
    "        for chain in PDBmodel:\n",
    "             for residue in chain:\n",
    "                     for atom in residue:\n",
    "                       \n",
    "                        Bfac=atom.get_bfactor()\n",
    "                        bfac_list.append(Bfac)\n",
    "                        avg_B=avg_B+Bfac\n",
    "                       \n",
    "    avg_B=avg_B/len (bfac_list)\n",
    "    print (f\"For {file}, average B-factor={avg_B}\")\n",
    "    plt.plot (bfac_list, label='lDDT')\n",
    "    plt.xlabel ('Atom #'   )\n",
    "    plt.ylabel ('iDDT')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return avg_B, bfac_list\n",
    "\n",
    "def sample_sequence_normalized_Bfac (seccs=[0.3, 0.3, 0.1, 0., 0., 0., 0., 0. ]):\n",
    "    sample_numbers=torch.tensor([seccs])\n",
    "    sample_numbers=torch.nn.functional.normalize (sample_numbers, dim=1)\n",
    "    sample_numbers=sample_numbers/torch.sum(sample_numbers)\n",
    "    \n",
    "    print (torch.sum (sample_numbers[0]), sample_numbers)\n",
    "    PDB=sample_sequence (model,\n",
    "                    X=sample_numbers,\n",
    "                     flag=0,cond_scales=1, foldproteins=True,\n",
    "                   )\n",
    "\n",
    "    avg,_ = get_avg_Bfac (file=PDB[0])\n",
    "\n",
    "    return PDB, avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7e6bb-7dca-43a6-b33a-1db755efc601",
   "metadata": {},
   "source": [
    "### DDSP analysis and error calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a5fd6-8522-43a3-8421-adfe425f0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_DSSP_result (fname):\n",
    "    pdb_list = [fname]\n",
    "\n",
    "    # parse structure\n",
    "    p = PDBParser()\n",
    "    for i in pdb_list:\n",
    "        structure = p.get_structure(i, fname)\n",
    "        # use only the first model\n",
    "        model = structure[0]\n",
    "        # calculate DSSP\n",
    "        dssp = DSSP(model, fname, file_type='PDB' )\n",
    "        # extract sequence and secondary structure from the DSSP tuple\n",
    "        sequence = ''\n",
    "        sec_structure = ''\n",
    "        for z in range(len(dssp)):\n",
    "            a_key = list(dssp.keys())[z]\n",
    "            sequence += dssp[a_key][1]\n",
    "            sec_structure += dssp[a_key][2]\n",
    "\n",
    "        #print(i)\n",
    "        #print(sequence)\n",
    "        #print(sec_structure)\n",
    "        #\n",
    "        # The DSSP codes for secondary structure used here are:\n",
    "        # =====     ====\n",
    "        # Code      Structure\n",
    "        # =====     ====\n",
    "        # H         Alpha helix (4-12)\n",
    "        # B         Isolated beta-bridge residue\n",
    "        # E         Strand\n",
    "        # G         3-10 helix\n",
    "        # I         Pi helix\n",
    "        # T         Turn\n",
    "        # S         Bend\n",
    "        # ~         None\n",
    "        # =====     ====\n",
    "        #\n",
    "        \n",
    "        sec_structure = sec_structure.replace('-', '~')\n",
    "        sec_structure_3state=sec_structure\n",
    "\n",
    "        \n",
    "        # if desired, convert DSSP's 8-state assignments into 3-state [C - coil, E - extended (beta-strand), H - helix]\n",
    "        sec_structure_3state = sec_structure_3state.replace('H', 'H') #0\n",
    "        sec_structure_3state = sec_structure_3state.replace('E', 'E')\n",
    "        sec_structure_3state = sec_structure_3state.replace('T', '~')\n",
    "        sec_structure_3state = sec_structure_3state.replace('~', '~')\n",
    "        sec_structure_3state = sec_structure_3state.replace('B', 'E')\n",
    "        sec_structure_3state = sec_structure_3state.replace('G', 'H') #5\n",
    "        sec_structure_3state = sec_structure_3state.replace('I', 'H') #6\n",
    "        sec_structure_3state = sec_structure_3state.replace('S', '~')\n",
    "        return sec_structure,sec_structure_3state, sequence\n",
    "    \n",
    "    \n",
    "def string_diff (seq1, seq2):    \n",
    "    return   sum(1 for a, b in zip(seq1, seq2) if a != b) + abs(len(seq1) - len(seq2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bfd5b2-2a2a-4ada-bc9a-b56400d90b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Model_A_error (fname, cond, plotit=True, ploterror=False):\n",
    "    \n",
    "    sec_structure,sec_structure_3state, sequence=get_DSSP_result (fname)\n",
    "    sscount=[]\n",
    "    length = len (sec_structure)\n",
    "    sscount.append (sec_structure.count('H')/length)\n",
    "    sscount.append (sec_structure.count('E')/length)\n",
    "    sscount.append (sec_structure.count('T')/length)\n",
    "    sscount.append (sec_structure.count('~')/length)\n",
    "    sscount.append (sec_structure.count('B')/length)\n",
    "    sscount.append (sec_structure.count('G')/length)\n",
    "    sscount.append (sec_structure.count('I')/length)\n",
    "    sscount.append (sec_structure.count('S')/length)\n",
    "    sscount=np.asarray (sscount)\n",
    "    \n",
    "    error=np.abs(sscount-cond)\n",
    "    print (\"Abs error per SS structure type (H, E, T, ~, B, G, I S): \", error)\n",
    "\n",
    "    if ploterror:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6,3))\n",
    "        plt.plot (error, 'o-', label='Error over SS type')\n",
    "        plt.legend()\n",
    "        plt.ylabel ('SS content')\n",
    "        plt.show()\n",
    "\n",
    "    x=np.linspace (0, 7, 8)\n",
    "    \n",
    "    sslabels=['H','E','T','~','B','G','I','S']\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6,3))\n",
    "    \n",
    "    ax.bar(x-0.15, cond, width=0.3, color='b', align='center')\n",
    "    ax.bar(x+0.15, sscount, width=0.3, color='r', align='center')\n",
    "   \n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    plt.xticks(range(len(sslabels)), sslabels, size='medium')\n",
    "    plt.legend (['GT','Prediction'])\n",
    "    \n",
    "    plt.ylabel ('SS content')\n",
    "    plt.show()\n",
    "    \n",
    "######################## 3 types\n",
    "\n",
    "    sscount=[]\n",
    "    length = len (sec_structure)\n",
    "    sscount.append (sec_structure_3state.count('H')/length)\n",
    "    sscount.append (sec_structure_3state.count('E')/length)\n",
    "    sscount.append (sec_structure_3state.count('~')/length)\n",
    "    cond_p=[np.sum([cond[0],cond[5], cond[6]]), np.sum ([cond[1], cond[4]]), np.sum([cond[2],cond[3],cond[7]]) ] \n",
    "                   \n",
    "    print (\"cond 3type: \",cond_p)\n",
    "    sscount=np.asarray (sscount)\n",
    "    \n",
    "    error3=np.abs(sscount-cond_p)\n",
    "    print (\"Abs error per 3-type SS structure type (C, H, E): \", error)\n",
    "    \n",
    "    if ploterror:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6,3))\n",
    "\n",
    "        plt.plot (error3, 'o-', label='Error over SS type')\n",
    "        plt.legend()\n",
    "        plt.ylabel ('SS content')\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    x=np.linspace (0,2, 3)\n",
    "    \n",
    "    sslabels=['H','E', '~' ]\n",
    "    \n",
    "    #ax = plt.subplot(111, figsize=(4,4))\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6,3))\n",
    "    \n",
    "                  \n",
    "    ax.bar(x-0.15, cond_p, width=0.3, color='b', align='center')\n",
    "    ax.bar(x+0.15, sscount, width=0.3, color='r', align='center')\n",
    "   \n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    plt.xticks(range(len(sslabels)), sslabels, size='medium')\n",
    "    plt.legend (['GT','Prediction'])\n",
    "    \n",
    "    plt.ylabel ('SS content')\n",
    "    plt.show()\n",
    "    \n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3929753a",
   "metadata": {},
   "source": [
    "## Define model and train or load weights, inference, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb118289",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='./output_model_A/'\n",
    "if not os.path.exists(prefix):\n",
    "        os.mkdir (prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d19d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_dim_position=128\n",
    "pred_dim=1\n",
    "cond_dim = 512\n",
    "model_A =ProteinDesigner_A(timesteps=(96), dim=768, pred_dim=pred_dim, \n",
    "                     loss_type=0, elucidated=True,\n",
    "                  padding_idx=0,\n",
    "                cond_dim = cond_dim,\n",
    "                text_embed_dim = cond_dim-embed_dim_position,\n",
    "                 embed_dim_position=128,\n",
    "                 max_text_len=8,\n",
    "             device=device,\n",
    "                )  .to(device)  \n",
    "\n",
    "params (model_A)\n",
    "params (model_A.imagen.unets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet_number=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e54e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_model=False #do not train if false\n",
    "\n",
    "if train_model:\n",
    "\n",
    "    trainer = ImagenTrainer(model)\n",
    "    train_loop (model,\n",
    "                    train_loader,\n",
    "                   test_loader,\n",
    "                    optimizer=None,\n",
    "                    print_every=100,\n",
    "                    epochs= 2400,\n",
    "                    start_ep=0,\n",
    "                start_step=0,\n",
    "                    train_unet_number=1,\n",
    "                print_loss =    50*len (train_loader)-1,\n",
    "                trainer=trainer,\n",
    "                plot_unscaled=False,#if unscaled data is plotted\n",
    "                max_batch_size =16,#if trainer....\n",
    "                save_model=True,\n",
    "                cond_scales=[1.], \n",
    "                num_samples=1,foldproteins=True,\n",
    "                   )\n",
    "    \n",
    "else:\n",
    "    #fname=f\"{prefix}model_A_final.pt\"  #Final checkpoint\n",
    "    fname=f\"{prefix}model_A_early.pt\"  #Early stopping checkpoint\n",
    "    model_A.load_state_dict(torch.load(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bad966",
   "metadata": {},
   "source": [
    "### Generative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e1fd2-1940-42b5-81a1-47e5ea0debf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_loop (model_A,\n",
    "                test_loader,\n",
    "                cond_scales=[1.], #list of cond scales - each sampled...\n",
    "                num_samples=4, #how many samples produced every time tested.....\n",
    "                timesteps=96, flag=10000,foldproteins=True,\n",
    "                calc_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979488cc-b2c4-4b30-870e-119e551c3add",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm_flag = False\n",
    "flag_ref=40000\n",
    "\n",
    "sample_sequence (model_A,\n",
    "                X=[[0, 0.7, 0.07, 0.1, 0.01, 0.02, 0.01, 0.11]], \n",
    "                    normalize_input_to_one=norm_flag,\n",
    "                 flag=flag_ref,cond_scales=1.,foldproteins=True,calc_error=True,\n",
    "               )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e05be-12d8-461a-a322-69561029afbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Generate candidates\n",
    "\n",
    "norm_flag = False\n",
    "flag_ref=40000\n",
    "\n",
    "sample_sequence (model_A,\n",
    "                X=[[0, 0.7, 0.07, 0.1, 0.01, 0.02, 0.01, 0.11]], \n",
    "                    normalize_input_to_one=norm_flag,\n",
    "                 flag=flag_ref,cond_scales=1.,foldproteins=True,calc_error=True,\n",
    "               )\n",
    "sample_sequence (model_A,\n",
    "                X=[[0.2, 0.2, 0.07, 0.3, 0.01, 0.02, 0.01, 0.11]], \n",
    "                    normalize_input_to_one=norm_flag,\n",
    "                 flag=flag_ref+1,cond_scales=1.,foldproteins=True,calc_error=True,\n",
    "               )\n",
    "\n",
    "sample_sequence (model_A,\n",
    "                X=[[0.8, 0.0, 0.0, 0., 0.0, 0.0,0.0, 0.]], \n",
    "                    normalize_input_to_one=norm_flag,\n",
    "                 flag=flag_ref+2,cond_scales=1.,foldproteins=True,calc_error=True,\n",
    "               )\n",
    "\n",
    "sample_sequence (model_A,\n",
    "                X=[[0.5, 0.0, 0.0, 0.2, 0.0, 0.0, 0.1, 0.1]], \n",
    "                    normalize_input_to_one=norm_flag,\n",
    "                 flag=flag_ref+3,cond_scales=1.,foldproteins=True,calc_error=True,\n",
    "               )\n",
    "\n",
    "sample_sequence (model_A,\n",
    "                X=[[0.01, 0.1, 0.0, 0.5, 0.0, 0.01, 0.1, 0.5 ]], \n",
    "                    normalize_input_to_one=norm_flag,\n",
    "                 flag=flag_ref+4,cond_scales=1.,foldproteins=True,calc_error=True,\n",
    "               )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
